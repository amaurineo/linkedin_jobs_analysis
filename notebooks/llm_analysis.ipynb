{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry run pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'Qwen/Qwen1.5-1.8B'  # Smaller and instruction-tuned\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-7B'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pschm\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-1.8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_qwen(job_description: str) -> dict:\n",
    "    \"\"\"Optimized extraction function with better prompt engineering\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at extracting required skills from job descriptions. \n",
    "    Given the job description below from a data related position, identify all the technical and professional skills mentioned. \n",
    "    Return the skills as a JSON object with the key \"skills\" containing a list of strings written in pt-br.\n",
    "    Do not include irrelevant information or explanations and only include skills that you consider essential to the data area.\n",
    "    \n",
    "\n",
    "    Job Description:\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": job_description[:2000]}\n",
    "    ]\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate output\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        # top_p=0.9,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # Extract and clean output\n",
    "    raw_output = outputs[0]['generated_text'].strip()\n",
    "    json_str = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        if \"skills\" in result and isinstance(result[\"skills\"], list):\n",
    "            return result\n",
    "        else:\n",
    "            return {\"error\": \"Invalid JSON structure\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Aqui na Ã­lia nossas vagas estÃ£o sempre abertas...\n",
       "1       vem construir o ItaÃº\\nQuer impulsionar o cresc...\n",
       "2       Job Description\\nA Globo Ã© feita de gente que ...\n",
       "3       Sobre o PicPay\\nCom mais de dez anos de histÃ³r...\n",
       "4       NÃ³s, da Knewin, somos lÃ­deres na AmÃ©rica Latin...\n",
       "                              ...                        \n",
       "1147    EstÃ¡gio na kinea para estudantes de economia c...\n",
       "1148    Ã‰ uma pessoa apaixonada por tecnologia e quer ...\n",
       "1149    Estamos em busca de Cinemarkers para fazer par...\n",
       "1150    Sobre o Bradesco\\nO Bradesco Ã© um dos maiores ...\n",
       "1151    DESCRIÃ‡ÃƒO\\nSomos uma empresa lÃ­der no ramo de ...\n",
       "Name: job_description, Length: 1152, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')['job_description']\n",
    "job_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')\n",
    "job_descriptions['job_description'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Failed to parse JSON output'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_requirements_qwen(job_descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\", \n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Databases & Data Storage\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\", \n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\", \n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Data Processing & ETL\n",
    "    \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\", \n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\", \n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Data Build Tool)\", \"Great Expectations\", \n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Business Intelligence & Data Visualization\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\", \n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\", \n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \n",
    "\n",
    "    # Machine Learning & AI\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \n",
    "    \"Hugging Face Transformers\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\", \n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Weights & Biases\", \n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Natural Language Processing (NLP)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\", \n",
    "    \"SentenceTransformers\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Computer Vision\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\", \n",
    "    \"TensorFlow Object Detection API\", \"Albumentations\", \"TorchVision\",\n",
    "\n",
    "    # Big Data & Distributed Computing\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\", \n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps & Model Deployment\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\", \n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\", \n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Cloud Platforms\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\", \n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\", \n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Statistical & Mathematical Foundations\n",
    "    \"Linear Algebra\", \"Calculus\", \"Probability\", \"Bayesian Statistics\", \"Hypothesis Testing\", \n",
    "    \"Descriptive Statistics\", \"Inferential Statistics\", \"Optimization\", \"Time Series Analysis\", \n",
    "    \"Markov Chains\", \"Graph Theory\", \"Stochastic Processes\",\n",
    "\n",
    "    # Feature Engineering\n",
    "    \"Feature Selection\", \"Feature Scaling\", \"Dimensionality Reduction\", \"PCA\", \"t-SNE\", \n",
    "    \"UMAP\", \"One-Hot Encoding\", \"Label Encoding\", \"Ordinal Encoding\", \"Target Encoding\", \n",
    "    \"Feature Crossing\", \"Polynomial Features\",\n",
    "\n",
    "    # Model Explainability & Fairness\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facets\", \"Responsible AI Toolkit\", \n",
    "    \"What-If Tool\",\n",
    "\n",
    "    # Time Series Analysis\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Graph Data Science\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\", \n",
    "\n",
    "    # Reinforcement Learning\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Data Governance & Privacy\n",
    "    \"GDPR Compliance\", \"CCPA Compliance\", \"Data Masking\", \"Differential Privacy\", \"Anonymization\", \n",
    "    \"Data Lineage\", \"Data Cataloging\", \"Data Security\", \"Data Contracts\", \"Data Quality Management\",\n",
    "\n",
    "    # Data Engineering\n",
    "    \"ETL Pipelines\", \"Data Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\", \n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\", \n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentation & A/B Testing\n",
    "    \"A/B Testing\", \"Multivariate Testing\", \"Bandit Algorithms\", \"Bayesian Optimization\", \n",
    "    \"Causal Inference\", \"DoWhy\",\n",
    "\n",
    "    # Optimization & Operations Research\n",
    "    \"Linear Programming\", \"Mixed-Integer Programming\", \"Convex Optimization\", \"Simulated Annealing\", \n",
    "    \"Genetic Algorithms\", \"Constraint Programming\",\n",
    "\n",
    "    # Data Ethics & Bias\n",
    "    \"Algorithmic Fairness\", \"Bias Detection\", \"Interpretability\", \"Explainable AI (XAI)\", \"Fair AI\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Data Cleaning\", \"Data Transformation\", \n",
    "    \"Data Streaming\", \"Feature Store\", \"Real-Time Analytics\", \"DataOps\", \"Observability\",\n",
    "\n",
    "    # Portuguese Translations (Extra Entries)\n",
    "    \"Aprendizado de MÃ¡quina\", \"Engenharia de Dados\", \"AnÃ¡lise de Dados\", \"Processamento de Linguagem Natural\", \n",
    "    \"CiÃªncia de Dados\", \"VisualizaÃ§Ã£o de Dados\", \"EstatÃ­stica\", \"OtimizaÃ§Ã£o\", \"Big Data\", \n",
    "    \"GovernanÃ§a de Dados\", \"OrquestraÃ§Ã£o de Dados\", \"Explicabilidade de Modelos\", \"SÃ©ries Temporais\",\n",
    "\n",
    "    # Linguagens de programaÃ§Ã£o\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\",\n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Bancos de dados e armazenamento de dados\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\",\n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\",\n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Processamento de dados e ETL\n",
    "    \"Excel\", \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\",\n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\",\n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Ferramenta de construÃ§Ã£o de dados)\", \"Great Expectations\",\n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # InteligÃªncia de negÃ³cios e visualizaÃ§Ã£o de dados\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\",\n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\",\n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \"PowerPoint\", \"Word\"\n",
    "\n",
    "    # Aprendizado de mÃ¡quina e IA\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\",\n",
    "    \"Transformadores de rostos abraÃ§ados\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\",\n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Pesos e vieses\",\n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Processamento de linguagem natural (PLN)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\",\n",
    "    \"Transformadores de frases\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # VisÃ£o computacional\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\",\n",
    "    \"API de detecÃ§Ã£o de objetos TensorFlow\", \"AlbumentaÃ§Ãµes\", \"TorchVision\",\n",
    "\n",
    "    # Big Data e computaÃ§Ã£o distribuÃ­da\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\",\n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps e implantaÃ§Ã£o de modelo\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\",\n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\",\n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Plataformas de nuvem\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\",\n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\",\n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Fundamentos estatÃ­sticos e matemÃ¡ticos\n",
    "    \"Ãlgebra linear\", \"CÃ¡lculo\", \"Probabilidade\", \"EstatÃ­stica bayesiana\", \"Teste de hipÃ³teses\",\n",
    "    \"EstatÃ­stica descritiva\", \"EstatÃ­stica inferencial\", \"OtimizaÃ§Ã£o\", \"AnÃ¡lise de sÃ©ries temporais\",\n",
    "    \"Cadeias de Markov\", \"Teoria dos grafos\", \"Processos estocÃ¡sticos\",\n",
    "\n",
    "    # Engenharia de recursos\n",
    "    \"SeleÃ§Ã£o de recursos\", \"Escalonamento de recursos\", \"ReduÃ§Ã£o de dimensionalidade\", \"PCA\", \"t-SNE\",\n",
    "    \"UMAP\", \"CodificaÃ§Ã£o One-Hot\", \"CodificaÃ§Ã£o de rÃ³tulos\", \"CodificaÃ§Ã£o ordinal\", \"CodificaÃ§Ã£o de destino\",\n",
    "    \"Cruzamento de recursos\", \"Recursos polinomiais\",\n",
    "\n",
    "    # Explicabilidade e imparcialidade do modelo\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facetas\", \"Kit de ferramentas de IA responsÃ¡vel\",\n",
    "    \"Ferramenta What-If\",\n",
    "\n",
    "    # AnÃ¡lise de sÃ©ries temporais\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # CiÃªncia de dados de grÃ¡fico\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\",\n",
    "\n",
    "    # Aprendizado por reforÃ§o\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # GovernanÃ§a e privacidade de dados\n",
    "    \"Conformidade com GDPR\", \"Conformidade com CCPA\", \"Mascaramento de dados\", \"Privacidade diferencial\", \"AnonimizaÃ§Ã£o\",\n",
    "    \"Linhagem de dados\", \"CatalogaÃ§Ã£o de dados\", \"SeguranÃ§a de dados\", \"Contratos de dados\", \"Gerenciamento de qualidade de dados\",\n",
    "\n",
    "    # Engenharia de dados\n",
    "    \"Pipelines ETL\", \"Dados Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\",\n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\",\n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # ExperimentaÃ§Ã£o e Teste A/B\n",
    "    \"Teste A/B\", \"Teste Multivariado\", \"Algoritmos Bandit\", \"OtimizaÃ§Ã£o Bayesiana\",\n",
    "    \"InferÃªncia Causal\", \"DoWhy\",\n",
    "\n",
    "    # OtimizaÃ§Ã£o e Pesquisa Operacional\n",
    "    \"ProgramaÃ§Ã£o Linear\", \"ProgramaÃ§Ã£o Inteira Mista\", \"OtimizaÃ§Ã£o Convexa\", \"Simulated Annealing\",\n",
    "    \"Algoritmos GenÃ©ticos\", \"ProgramaÃ§Ã£o de RestriÃ§Ãµes\",\n",
    "\n",
    "    # Ã‰tica e ViÃ©s de Dados\n",
    "    \"JustiÃ§a AlgorÃ­tmica\", \"DetecÃ§Ã£o de ViÃ©s\", \"Interpretabilidade\", \"IA ExplicÃ¡vel (XAI)\", \"IA Justa\",\n",
    "\n",
    "    # Diversos\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Limpeza de dados\", \"TransformaÃ§Ã£o de dados\", \n",
    "    \"Streaming de dados\", \"Feature Store\", \"AnÃ¡lise em tempo real\", \"DataOps\", \"Observabilidade\",\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages / Tools\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"SQL\",\n",
    "    \"Excel\",\n",
    "\n",
    "    # Machine Learning & Artificial Intelligence\n",
    "    \"Machine Learning\", \"Aprendizagem de MÃ¡quina\",\n",
    "    \"Deep Learning\", \"Aprendizagem Profunda\",\n",
    "    \"Reinforcement Learning\", \"Aprendizagem por ReforÃ§o\",\n",
    "    \"Artificial Intelligence\", \"InteligÃªncia Artificial\",\n",
    "    \"Algorithm Development\", \"Desenvolvimento de Algoritmos\",\n",
    "\n",
    "    # Data Analysis & Statistics\n",
    "    \"Data Analysis\", \"AnÃ¡lise de Dados\",\n",
    "    \"Statistical Analysis\", \"AnÃ¡lise EstatÃ­stica\",\n",
    "    \"Statistics\", \"EstatÃ­stica\",\n",
    "    \"Predictive Analytics\", \"AnÃ¡lise Preditiva\",\n",
    "    \"Prescriptive Analytics\", \"AnÃ¡lise Prescritiva\",\n",
    "    \"Statistical Modeling\", \"Modelagem EstatÃ­stica\",\n",
    "    \n",
    "    # Data Science & Engineering\n",
    "    \"Data Science\", \"CiÃªncia de Dados\",\n",
    "    \"Data Engineering\", \"Engenharia de Dados\",\n",
    "    \"Data Architecture\", \"Arquitetura de Dados\",\n",
    "    \"Data Strategy\", \"EstratÃ©gia de Dados\",\n",
    "    \n",
    "    # Data Visualization & Storytelling\n",
    "    \"Data Visualization\", \"VisualizaÃ§Ã£o de Dados\",\n",
    "    \"Data Storytelling\", \"ContaÃ§Ã£o de HistÃ³rias com Dados\",\n",
    "    \"Business Intelligence\", \"InteligÃªncia de NegÃ³cios\",\n",
    "    \"Business Analytics\", \"AnÃ¡lise de NegÃ³cios\",\n",
    "    \n",
    "    # Big Data & Cloud\n",
    "    \"Big Data\", \"Big Data\",\n",
    "    \"Data Warehousing\", \"Armazenamento de Dados\",\n",
    "    \"Big Data Engineering\", \"Engenharia de Big Data\",\n",
    "    \"Big Data Analytics\", \"AnÃ¡lise de Big Data\",\n",
    "    \"Cloud Computing\", \"ComputaÃ§Ã£o em Nuvem\",\n",
    "    \"Cloud Data Services\", \"ServiÃ§os de Dados em Nuvem\",\n",
    "    \n",
    "    # Data Processing & Tools\n",
    "    \"ETL\",\n",
    "    \"Data Pipelines\", \"Fluxos de Dados\",\n",
    "    \"Data Integration\", \"IntegraÃ§Ã£o de Dados\",\n",
    "    \"Data Cleaning\", \"Limpeza de Dados\",\n",
    "    \"Data Wrangling\", \"ManipulaÃ§Ã£o de Dados\",\n",
    "    \"Data Quality\", \"Qualidade dos Dados\",\n",
    "    \"Data Governance\", \"GovernanÃ§a de Dados\",\n",
    "    \"Data Platforms\", \"Plataformas de Dados\",\n",
    "    \n",
    "    # Databases & Query Optimization\n",
    "    \"NoSQL\",\n",
    "    \"SQL Tuning\", \"OtimizaÃ§Ã£o de SQL\",\n",
    "    \"Query Optimization\", \"OtimizaÃ§Ã£o de Consultas\",\n",
    "    \n",
    "    # Big Data Frameworks\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \n",
    "    # Specialized Tools & Software\n",
    "    \"Tableau\",\n",
    "    \"Power BI\",\n",
    "    \"SAS\",\n",
    "    \"Stata\",\n",
    "    \"Matlab\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \n",
    "    # Advanced Data Concepts\n",
    "    \"Feature Engineering\", \"Engenharia de Atributos\",\n",
    "    \"Time Series Analysis\", \"AnÃ¡lise de SÃ©ries Temporais\",\n",
    "    \"A/B Testing\", \"Testes A/B\",\n",
    "    \"Data Mining Techniques\", \"TÃ©cnicas de MineraÃ§Ã£o de Dados\",\n",
    "    \"Data Mining\", \"MineraÃ§Ã£o de Dados\",\n",
    "    \"Natural Language Processing\", \"Processamento de Linguagem Natural\",\n",
    "    \"Computer Vision\", \"VisÃ£o Computacional\",\n",
    "    \"Information Retrieval\", \"RecuperaÃ§Ã£o de InformaÃ§Ã£o\",\n",
    "    \n",
    "    # Security & Ethics\n",
    "    \"Data Security\", \"SeguranÃ§a de Dados\",\n",
    "    \"Data Privacy\", \"Privacidade de Dados\",\n",
    "    \"Data Ethics\", \"Ã‰tica de Dados\"\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "# skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'Excel', 'PowerPoint', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"O estagiÃ¡rio(a) de Sistemas da InformaÃ§Ã£o serÃ¡ responsÃ¡vel por prestar suporte aos usuÃ¡rios do ERP Datasul, atuando diretamente na resoluÃ§Ã£o de problemas e na abertura e acompanhamento de chamados com a empresa fornecedora do software. O profissional tambÃ©m participarÃ¡ de treinamentos, reuniÃµes de equipe.\n",
    "Responsabilidades e atribuiÃ§Ãµes\n",
    "Prestar suporte tÃ©cnico e funcional aos usuÃ¡rios do ERP Datasul.\n",
    "Auxiliar na resoluÃ§Ã£o de problemas relacionados ao sistema, garantindo o bom funcionamento e a continuidade das operaÃ§Ãµes.\n",
    "Realizar a abertura e acompanhamento de chamados tÃ©cnicos junto Ã  empresa fornecedora do software, garantindo que as solicitaÃ§Ãµes sejam tratadas de maneira eficiente.\n",
    "Participar de treinamentos contÃ­nuos para atualizaÃ§Ã£o sobre novas funcionalidades e melhorias no sistema.\n",
    "Colaborar com a equipe nas reuniÃµes para discutir melhorias, ajustes e necessidades do sistema.\n",
    "Atuar no suporte Ã s Ã¡reas ContÃ¡bil, Fiscal, Administrativa e Financeira, garantindo o alinhamento entre os processos e o ERP.\n",
    "Requisitos e qualificaÃ§Ãµes\n",
    "Cursando graduaÃ§Ã£o em Sistemas de InformaÃ§Ã£o, CiÃªncias da ComputaÃ§Ã£o ou Ã¡reas relacionadas.\n",
    "Conhecimento bÃ¡sico em sistemas ERP (preferencialmente Datasul).\n",
    "Habilidade em comunicaÃ§Ã£o para interaÃ§Ã£o com usuÃ¡rios e fornecedores.\n",
    "Proatividade, organizaÃ§Ã£o e vontade de aprender.\n",
    "Leitura de manuais, documentaÃ§Ã£o e materiais tÃ©cnicos para suporte ao usuÃ¡rio e resoluÃ§Ã£o de problemas\n",
    "Entendimento do ciclo de vida e operaÃ§Ã£o de sistemas computacionais.\n",
    "NoÃ§Ãµes bÃ¡sicas de bancos de dados relacionais e nÃ£o relacionais.\n",
    "Conhecimentos bÃ¡sicos de consultas e manipulaÃ§Ã£o de dados em SQL.\n",
    "Habilidade em ferramentas como Word, Excel e PowerPoint.\"\n",
    "7,4075025088,FaÃ§a sua Carreira de Engenharia de Processos no ItaÃº ðŸš€ðŸ§¡,ItaÃº Unibanco,HÃ­brido,\"SÃ£o Paulo, SP\",HÃ¡ 2 semanas,,Assistente,Tempo integral,Bancos,\"Vem construir no ItaÃº_\n",
    "Como maior banco privado da AmÃ©rica Latina, acompanhamos de perto o desenvolvimento de novas tecnologias e enxergamos o processo como uma evoluÃ§Ã£o natural do nosso negÃ³cio, que\n",
    "torna possÃ­vel termos um contato ainda mais prÃ³ximo com a experiÃªncia de nossos clientes\n",
    "para continuarmos aprimorando suas jornadas.\n",
    "Como Ã© a Ã¡rea de OperaÃ§Ãµes?\n",
    "Ao longo de nossa histÃ³ria, desempenhamos um\n",
    "papel fundamental na estratÃ©gia do ItaÃº Unibanco\n",
    ". Dentro deste contexto, todos os produtos que oferecemos aos nossos clientes passam a ser produtos tecnolÃ³gicos, independente da interface que ele escolha â€“ seja ela fÃ­sica, digital ou mista. Vem construir soluÃ§Ãµes digitais em uma das maiores instituiÃ§Ãµes financeiras da AmÃ©rica Latina!\n",
    "Como Ã© a Carreira de Engenharia de Processos?\n",
    "A Engenharia de processos no ItaÃº exerce um papel fundamental na busca constante da melhor jornada para nossos clientes, vem pra repensar o fundamental (ir na causa raiz das coisas,) buscar a reestruturaÃ§Ã£o radical dos processos que visam alcanÃ§ar mudanÃ§as disruptivas em indicadores crÃ­ticos de desempenho, tais como qualidade, custos, riscos e agilidade. A pessoa engenheira de processos pode estar alocada em Squads, em comunidades ou nas Ã¡reas do banco atuando sempre em projetos de transformaÃ§Ã£o e melhoria contÃ­nua (Build e Run).\n",
    "Como serÃ¡ o seu dia a dia?\n",
    "Garantir a aplicabilidade dos mÃ©todos para transformaÃ§Ã£o das Jornadas fim a fim dos clientes.\n",
    "Priorizar as aÃ§Ãµes quantificando o impacto atravÃ©s de uma Engenharia de Valor, garantindo assertividade nas escolhas que agregam.\n",
    "Atuar no mapeamento de processos, resoluÃ§Ã£o de problemas, construÃ§Ã£o e redesenho de jornadas, com foco no cliente, eficiÃªncia, qualidade e gestÃ£o de riscos.\n",
    "Desdobrar as dores dos clientes e conectar com elementos de processo.\n",
    "Realizar o estudo e a construÃ§Ã£o dos requisitos para os desenvolvimentos de tecnologia dos problemas assegurando o reuso de soluÃ§Ãµes.\n",
    "Desenvolver soluÃ§Ãµes em No/Low-Code para transformaÃ§Ã£o dos processos. Garante o teste funcional e homologaÃ§Ã£o de requisitos de processos nas soluÃ§Ãµes desenvolvidas\n",
    "Estrutura GestÃ£o e Controle dos processos , aplicando ferramentas de qualidade , planejamento de demanda, intervenÃ§Ãµes operacionais, process mining , gestÃ£o da rotina , entre outras.\n",
    "Modelo de trabalho: hÃ­brido de 8x presenciais no mÃªs\n",
    "_Quais sÃ£o as habilidades e competÃªncias necessÃ¡rias?\n",
    "Principais Habilidades TÃ©cnicas Com ExperiÃªncia PrÃ¡tica\n",
    "LEAN\n",
    "Qualidade\n",
    "PCP (Planejamento e Controle de ProduÃ§Ã£o)\n",
    "GestÃ£o de Projetos/ MÃ©todos Ãgeis\n",
    "AnÃ¡lise e exploraÃ§Ã£o de Dados\n",
    "Process Mining\n",
    "MÃ©todo de SoluÃ§Ã£o de Problemas\n",
    "FormaÃ§Ã£o em Engenharia de ProduÃ§Ã£o Ã© um diferencial\n",
    "_O que esperamos de vocÃª:\n",
    "Habilidade em criticar de forma construtiva e provocativa;\n",
    "Habilidade em negociaÃ§Ã£o e construÃ§Ã£o de parcerias;\n",
    "Capacidade analÃ­tica/ lÃ³gica\n",
    "Habilidade em conectar conhecimento tÃ©cnico com a estratÃ©gia dos projetos.\n",
    "OrganizaÃ§Ã£o, disciplina e prÃ³-atividade\n",
    "Habilidade em trabalhar em equipe de forma colaborativa,\n",
    "Capacidade de tomada de decisÃµes com base em processos e dados, foco no cliente e resultados;\n",
    "ðŸ§¡\n",
    "\"\"\"\n",
    "\n",
    "# Load Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Convert skill list into patterns\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Find matches\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "print(\"Extracted Skills:\", set(skills_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'arquitetura de dados', 'SQL', 'anÃ¡lise de dados', 'CiÃªncia de Dados', 'InteligÃªncia Artificial', 'EstatÃ­stica'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process  # Fuzzy matching\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"DescriÃ§Ã£o\n",
    "Estamos em busca de um(a) estagiÃ¡rio(a) para integrar nosso time de dados, focado na automatizaÃ§Ã£o e anÃ¡lise de dados, contribuindo diretamente para a tomada de decisÃ£o estratÃ©gica da empresa.\n",
    "Responsabilidades e atribuiÃ§Ãµes\n",
    "Atuar na construÃ§Ã£o, governanÃ§a e manutenÃ§Ã£o da arquitetura de dados do RPA.\n",
    "Automatizar processo e otimizaÃ§Ã£o para reduzir esforÃ§os operacionais repetitivos.\n",
    "Promover a cultura de dados dentro da empresa e junto aos clientes, auxiliando em projetos em andamento.\n",
    "Participar da concessÃ£o, estruturaÃ§Ã£o e organizaÃ§Ã£o de dados.\n",
    "Contribuir para projetos de InteligÃªncia Artificial (IA).\n",
    "Apoiar a construÃ§Ã£o da arquitetura de dados e o desenvolvimento de novas funcionalidades que promovem escalabilidade e eficiÃªncia.\n",
    "Requisitos e qualificaÃ§Ãµes\n",
    "Cursando graduaÃ§Ã£o em Tecnologia, Engenharia, EstatÃ­stica, CiÃªncia de Dados ou Ã¡reas correlatas.\n",
    "NoÃ§Ãµes de SQL para manipulaÃ§Ã£o e extraÃ§Ã£o de dados.\n",
    "Familiaridade com ferramentas de BI (ex.: Metabase, powerbi).\n",
    "ProgramaÃ§Ã£o em Phyton e uso de bibliotecas para anÃ¡lise de dados (diferencial).\n",
    "Conhecimento em ferramentas de RPA (diferencial).\n",
    "Capacidade de levantamento e acompanhamento de KPIs em colaboraÃ§Ã£o com usuÃ¡rios.\"\"\"\n",
    "\n",
    "# Load Portuguese NLP model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Create PhraseMatcher for exact matches\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Extract exact matches using PhraseMatcher\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "# Extract all words from the text\n",
    "words = set(re.findall(r\"\\b\\w+\\b\", sample_desc))  # Get all unique words\n",
    "\n",
    "# Apply fuzzy matching for words with a similarity score above a threshold (80%)\n",
    "for word in words:\n",
    "    best_match, score = process.extractOne(word, skill_list)\n",
    "    if score > 93:  # Adjust threshold as needed\n",
    "        skills_found.append(best_match)\n",
    "\n",
    "# Remove duplicates\n",
    "skills_found = set(skills_found)\n",
    "\n",
    "print(\"Extracted Skills:\", skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python', 83)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne('phyton', skill_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
