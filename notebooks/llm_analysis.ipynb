{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry run pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'Qwen/Qwen1.5-1.8B'  # Smaller and instruction-tuned\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-7B'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pschm\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-1.8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_qwen(job_description: str) -> dict:\n",
    "    \"\"\"Optimized extraction function with better prompt engineering\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at extracting required skills from job descriptions. \n",
    "    Given the job description below from a data related position, identify all the technical and professional skills mentioned. \n",
    "    Return the skills as a JSON object with the key \"skills\" containing a list of strings written in pt-br.\n",
    "    Do not include irrelevant information or explanations and only include skills that you consider essential to the data area.\n",
    "    \n",
    "\n",
    "    Job Description:\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": job_description[:2000]}\n",
    "    ]\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate output\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=False,\n",
    "        # temperature=0.7,\n",
    "        # top_p=0.9,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # Extract and clean output\n",
    "    raw_output = outputs[0]['generated_text'].strip()\n",
    "    json_str = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        if \"skills\" in result and isinstance(result[\"skills\"], list):\n",
    "            return result\n",
    "        else:\n",
    "            return {\"error\": \"Invalid JSON structure\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Aqui na ília nossas vagas estão sempre abertas...\n",
       "1       vem construir o Itaú\\nQuer impulsionar o cresc...\n",
       "2       Job Description\\nA Globo é feita de gente que ...\n",
       "3       Sobre o PicPay\\nCom mais de dez anos de histór...\n",
       "4       Nós, da Knewin, somos líderes na América Latin...\n",
       "                              ...                        \n",
       "1147    Estágio na kinea para estudantes de economia c...\n",
       "1148    É uma pessoa apaixonada por tecnologia e quer ...\n",
       "1149    Estamos em busca de Cinemarkers para fazer par...\n",
       "1150    Sobre o Bradesco\\nO Bradesco é um dos maiores ...\n",
       "1151    DESCRIÇÃO\\nSomos uma empresa líder no ramo de ...\n",
       "Name: job_description, Length: 1152, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')['job_description']\n",
    "job_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')\n",
    "job_descriptions['job_description'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Failed to parse JSON output'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_requirements_qwen(job_descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\", \n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Databases & Data Storage\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\", \n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\", \n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Data Processing & ETL\n",
    "    \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\", \n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\", \n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Data Build Tool)\", \"Great Expectations\", \n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Business Intelligence & Data Visualization\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\", \n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\", \n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \n",
    "\n",
    "    # Machine Learning & AI\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \n",
    "    \"Hugging Face Transformers\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\", \n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Weights & Biases\", \n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Natural Language Processing (NLP)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\", \n",
    "    \"SentenceTransformers\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Computer Vision\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\", \n",
    "    \"TensorFlow Object Detection API\", \"Albumentations\", \"TorchVision\",\n",
    "\n",
    "    # Big Data & Distributed Computing\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\", \n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps & Model Deployment\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\", \n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\", \n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Cloud Platforms\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\", \n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\", \n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Statistical & Mathematical Foundations\n",
    "    \"Linear Algebra\", \"Calculus\", \"Probability\", \"Bayesian Statistics\", \"Hypothesis Testing\", \n",
    "    \"Descriptive Statistics\", \"Inferential Statistics\", \"Optimization\", \"Time Series Analysis\", \n",
    "    \"Markov Chains\", \"Graph Theory\", \"Stochastic Processes\",\n",
    "\n",
    "    # Feature Engineering\n",
    "    \"Feature Selection\", \"Feature Scaling\", \"Dimensionality Reduction\", \"PCA\", \"t-SNE\", \n",
    "    \"UMAP\", \"One-Hot Encoding\", \"Label Encoding\", \"Ordinal Encoding\", \"Target Encoding\", \n",
    "    \"Feature Crossing\", \"Polynomial Features\",\n",
    "\n",
    "    # Model Explainability & Fairness\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facets\", \"Responsible AI Toolkit\", \n",
    "    \"What-If Tool\",\n",
    "\n",
    "    # Time Series Analysis\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Graph Data Science\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\", \n",
    "\n",
    "    # Reinforcement Learning\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Data Governance & Privacy\n",
    "    \"GDPR Compliance\", \"CCPA Compliance\", \"Data Masking\", \"Differential Privacy\", \"Anonymization\", \n",
    "    \"Data Lineage\", \"Data Cataloging\", \"Data Security\", \"Data Contracts\", \"Data Quality Management\",\n",
    "\n",
    "    # Data Engineering\n",
    "    \"ETL Pipelines\", \"Data Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\", \n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\", \n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentation & A/B Testing\n",
    "    \"A/B Testing\", \"Multivariate Testing\", \"Bandit Algorithms\", \"Bayesian Optimization\", \n",
    "    \"Causal Inference\", \"DoWhy\",\n",
    "\n",
    "    # Optimization & Operations Research\n",
    "    \"Linear Programming\", \"Mixed-Integer Programming\", \"Convex Optimization\", \"Simulated Annealing\", \n",
    "    \"Genetic Algorithms\", \"Constraint Programming\",\n",
    "\n",
    "    # Data Ethics & Bias\n",
    "    \"Algorithmic Fairness\", \"Bias Detection\", \"Interpretability\", \"Explainable AI (XAI)\", \"Fair AI\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Data Cleaning\", \"Data Transformation\", \n",
    "    \"Data Streaming\", \"Feature Store\", \"Real-Time Analytics\", \"DataOps\", \"Observability\",\n",
    "\n",
    "    # Portuguese Translations (Extra Entries)\n",
    "    \"Aprendizado de Máquina\", \"Engenharia de Dados\", \"Análise de Dados\", \"Processamento de Linguagem Natural\", \n",
    "    \"Ciência de Dados\", \"Visualização de Dados\", \"Estatística\", \"Otimização\", \"Big Data\", \n",
    "    \"Governança de Dados\", \"Orquestração de Dados\", \"Explicabilidade de Modelos\", \"Séries Temporais\",\n",
    "\n",
    "    # Linguagens de programação\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\",\n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Bancos de dados e armazenamento de dados\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\",\n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\",\n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Processamento de dados e ETL\n",
    "    \"Excel\", \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\",\n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\",\n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Ferramenta de construção de dados)\", \"Great Expectations\",\n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Inteligência de negócios e visualização de dados\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\",\n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\",\n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \"PowerPoint\", \"Word\"\n",
    "\n",
    "    # Aprendizado de máquina e IA\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\",\n",
    "    \"Transformadores de rostos abraçados\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\",\n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Pesos e vieses\",\n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Processamento de linguagem natural (PLN)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\",\n",
    "    \"Transformadores de frases\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Visão computacional\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\",\n",
    "    \"API de detecção de objetos TensorFlow\", \"Albumentações\", \"TorchVision\",\n",
    "\n",
    "    # Big Data e computação distribuída\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\",\n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps e implantação de modelo\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\",\n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\",\n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Plataformas de nuvem\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\",\n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\",\n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Fundamentos estatísticos e matemáticos\n",
    "    \"Álgebra linear\", \"Cálculo\", \"Probabilidade\", \"Estatística bayesiana\", \"Teste de hipóteses\",\n",
    "    \"Estatística descritiva\", \"Estatística inferencial\", \"Otimização\", \"Análise de séries temporais\",\n",
    "    \"Cadeias de Markov\", \"Teoria dos grafos\", \"Processos estocásticos\",\n",
    "\n",
    "    # Engenharia de recursos\n",
    "    \"Seleção de recursos\", \"Escalonamento de recursos\", \"Redução de dimensionalidade\", \"PCA\", \"t-SNE\",\n",
    "    \"UMAP\", \"Codificação One-Hot\", \"Codificação de rótulos\", \"Codificação ordinal\", \"Codificação de destino\",\n",
    "    \"Cruzamento de recursos\", \"Recursos polinomiais\",\n",
    "\n",
    "    # Explicabilidade e imparcialidade do modelo\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facetas\", \"Kit de ferramentas de IA responsável\",\n",
    "    \"Ferramenta What-If\",\n",
    "\n",
    "    # Análise de séries temporais\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Ciência de dados de gráfico\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\",\n",
    "\n",
    "    # Aprendizado por reforço\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Governança e privacidade de dados\n",
    "    \"Conformidade com GDPR\", \"Conformidade com CCPA\", \"Mascaramento de dados\", \"Privacidade diferencial\", \"Anonimização\",\n",
    "    \"Linhagem de dados\", \"Catalogação de dados\", \"Segurança de dados\", \"Contratos de dados\", \"Gerenciamento de qualidade de dados\",\n",
    "\n",
    "    # Engenharia de dados\n",
    "    \"Pipelines ETL\", \"Dados Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\",\n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\",\n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentação e Teste A/B\n",
    "    \"Teste A/B\", \"Teste Multivariado\", \"Algoritmos Bandit\", \"Otimização Bayesiana\",\n",
    "    \"Inferência Causal\", \"DoWhy\",\n",
    "\n",
    "    # Otimização e Pesquisa Operacional\n",
    "    \"Programação Linear\", \"Programação Inteira Mista\", \"Otimização Convexa\", \"Simulated Annealing\",\n",
    "    \"Algoritmos Genéticos\", \"Programação de Restrições\",\n",
    "\n",
    "    # Ética e Viés de Dados\n",
    "    \"Justiça Algorítmica\", \"Detecção de Viés\", \"Interpretabilidade\", \"IA Explicável (XAI)\", \"IA Justa\",\n",
    "\n",
    "    # Diversos\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Limpeza de dados\", \"Transformação de dados\", \n",
    "    \"Streaming de dados\", \"Feature Store\", \"Análise em tempo real\", \"DataOps\", \"Observabilidade\",\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages / Tools\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"SQL\",\n",
    "    \"Excel\",\n",
    "\n",
    "    # Machine Learning & Artificial Intelligence\n",
    "    \"Machine Learning\", \"Aprendizagem de Máquina\",\n",
    "    \"Deep Learning\", \"Aprendizagem Profunda\",\n",
    "    \"Reinforcement Learning\", \"Aprendizagem por Reforço\",\n",
    "    \"Artificial Intelligence\", \"Inteligência Artificial\",\n",
    "    \"Algorithm Development\", \"Desenvolvimento de Algoritmos\",\n",
    "\n",
    "    # Data Analysis & Statistics\n",
    "    \"Data Analysis\", \"Análise de Dados\",\n",
    "    \"Statistical Analysis\", \"Análise Estatística\",\n",
    "    \"Statistics\", \"Estatística\",\n",
    "    \"Predictive Analytics\", \"Análise Preditiva\",\n",
    "    \"Prescriptive Analytics\", \"Análise Prescritiva\",\n",
    "    \"Statistical Modeling\", \"Modelagem Estatística\",\n",
    "    \n",
    "    # Data Science & Engineering\n",
    "    \"Data Science\", \"Ciência de Dados\",\n",
    "    \"Data Engineering\", \"Engenharia de Dados\",\n",
    "    \"Data Architecture\", \"Arquitetura de Dados\",\n",
    "    \"Data Strategy\", \"Estratégia de Dados\",\n",
    "    \n",
    "    # Data Visualization & Storytelling\n",
    "    \"Data Visualization\", \"Visualização de Dados\",\n",
    "    \"Data Storytelling\", \"Contação de Histórias com Dados\",\n",
    "    \"Business Intelligence\", \"Inteligência de Negócios\",\n",
    "    \"Business Analytics\", \"Análise de Negócios\",\n",
    "    \n",
    "    # Big Data & Cloud\n",
    "    \"Big Data\", \"Big Data\",\n",
    "    \"Data Warehousing\", \"Armazenamento de Dados\",\n",
    "    \"Big Data Engineering\", \"Engenharia de Big Data\",\n",
    "    \"Big Data Analytics\", \"Análise de Big Data\",\n",
    "    \"Cloud Computing\", \"Computação em Nuvem\",\n",
    "    \"Cloud Data Services\", \"Serviços de Dados em Nuvem\",\n",
    "    \n",
    "    # Data Processing & Tools\n",
    "    \"ETL\",\n",
    "    \"Data Pipelines\", \"Fluxos de Dados\",\n",
    "    \"Data Integration\", \"Integração de Dados\",\n",
    "    \"Data Cleaning\", \"Limpeza de Dados\",\n",
    "    \"Data Wrangling\", \"Manipulação de Dados\",\n",
    "    \"Data Quality\", \"Qualidade dos Dados\",\n",
    "    \"Data Governance\", \"Governança de Dados\",\n",
    "    \"Data Platforms\", \"Plataformas de Dados\",\n",
    "    \n",
    "    # Databases & Query Optimization\n",
    "    \"NoSQL\",\n",
    "    \"SQL Tuning\", \"Otimização de SQL\",\n",
    "    \"Query Optimization\", \"Otimização de Consultas\",\n",
    "    \n",
    "    # Big Data Frameworks\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \n",
    "    # Specialized Tools & Software\n",
    "    \"Tableau\",\n",
    "    \"Power BI\",\n",
    "    \"SAS\",\n",
    "    \"Stata\",\n",
    "    \"Matlab\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \n",
    "    # Advanced Data Concepts\n",
    "    \"Feature Engineering\", \"Engenharia de Atributos\",\n",
    "    \"Time Series Analysis\", \"Análise de Séries Temporais\",\n",
    "    \"A/B Testing\", \"Testes A/B\",\n",
    "    \"Data Mining Techniques\", \"Técnicas de Mineração de Dados\",\n",
    "    \"Data Mining\", \"Mineração de Dados\",\n",
    "    \"Natural Language Processing\", \"Processamento de Linguagem Natural\",\n",
    "    \"Computer Vision\", \"Visão Computacional\",\n",
    "    \"Information Retrieval\", \"Recuperação de Informação\",\n",
    "    \n",
    "    # Security & Ethics\n",
    "    \"Data Security\", \"Segurança de Dados\",\n",
    "    \"Data Privacy\", \"Privacidade de Dados\",\n",
    "    \"Data Ethics\", \"Ética de Dados\"\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "# skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'Excel', 'PowerPoint', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"O estagiário(a) de Sistemas da Informação será responsável por prestar suporte aos usuários do ERP Datasul, atuando diretamente na resolução de problemas e na abertura e acompanhamento de chamados com a empresa fornecedora do software. O profissional também participará de treinamentos, reuniões de equipe.\n",
    "Responsabilidades e atribuições\n",
    "Prestar suporte técnico e funcional aos usuários do ERP Datasul.\n",
    "Auxiliar na resolução de problemas relacionados ao sistema, garantindo o bom funcionamento e a continuidade das operações.\n",
    "Realizar a abertura e acompanhamento de chamados técnicos junto à empresa fornecedora do software, garantindo que as solicitações sejam tratadas de maneira eficiente.\n",
    "Participar de treinamentos contínuos para atualização sobre novas funcionalidades e melhorias no sistema.\n",
    "Colaborar com a equipe nas reuniões para discutir melhorias, ajustes e necessidades do sistema.\n",
    "Atuar no suporte às áreas Contábil, Fiscal, Administrativa e Financeira, garantindo o alinhamento entre os processos e o ERP.\n",
    "Requisitos e qualificações\n",
    "Cursando graduação em Sistemas de Informação, Ciências da Computação ou áreas relacionadas.\n",
    "Conhecimento básico em sistemas ERP (preferencialmente Datasul).\n",
    "Habilidade em comunicação para interação com usuários e fornecedores.\n",
    "Proatividade, organização e vontade de aprender.\n",
    "Leitura de manuais, documentação e materiais técnicos para suporte ao usuário e resolução de problemas\n",
    "Entendimento do ciclo de vida e operação de sistemas computacionais.\n",
    "Noções básicas de bancos de dados relacionais e não relacionais.\n",
    "Conhecimentos básicos de consultas e manipulação de dados em SQL.\n",
    "Habilidade em ferramentas como Word, Excel e PowerPoint.\"\n",
    "7,4075025088,Faça sua Carreira de Engenharia de Processos no Itaú 🚀🧡,Itaú Unibanco,Híbrido,\"São Paulo, SP\",Há 2 semanas,,Assistente,Tempo integral,Bancos,\"Vem construir no Itaú_\n",
    "Como maior banco privado da América Latina, acompanhamos de perto o desenvolvimento de novas tecnologias e enxergamos o processo como uma evolução natural do nosso negócio, que\n",
    "torna possível termos um contato ainda mais próximo com a experiência de nossos clientes\n",
    "para continuarmos aprimorando suas jornadas.\n",
    "Como é a área de Operações?\n",
    "Ao longo de nossa história, desempenhamos um\n",
    "papel fundamental na estratégia do Itaú Unibanco\n",
    ". Dentro deste contexto, todos os produtos que oferecemos aos nossos clientes passam a ser produtos tecnológicos, independente da interface que ele escolha – seja ela física, digital ou mista. Vem construir soluções digitais em uma das maiores instituições financeiras da América Latina!\n",
    "Como é a Carreira de Engenharia de Processos?\n",
    "A Engenharia de processos no Itaú exerce um papel fundamental na busca constante da melhor jornada para nossos clientes, vem pra repensar o fundamental (ir na causa raiz das coisas,) buscar a reestruturação radical dos processos que visam alcançar mudanças disruptivas em indicadores críticos de desempenho, tais como qualidade, custos, riscos e agilidade. A pessoa engenheira de processos pode estar alocada em Squads, em comunidades ou nas áreas do banco atuando sempre em projetos de transformação e melhoria contínua (Build e Run).\n",
    "Como será o seu dia a dia?\n",
    "Garantir a aplicabilidade dos métodos para transformação das Jornadas fim a fim dos clientes.\n",
    "Priorizar as ações quantificando o impacto através de uma Engenharia de Valor, garantindo assertividade nas escolhas que agregam.\n",
    "Atuar no mapeamento de processos, resolução de problemas, construção e redesenho de jornadas, com foco no cliente, eficiência, qualidade e gestão de riscos.\n",
    "Desdobrar as dores dos clientes e conectar com elementos de processo.\n",
    "Realizar o estudo e a construção dos requisitos para os desenvolvimentos de tecnologia dos problemas assegurando o reuso de soluções.\n",
    "Desenvolver soluções em No/Low-Code para transformação dos processos. Garante o teste funcional e homologação de requisitos de processos nas soluções desenvolvidas\n",
    "Estrutura Gestão e Controle dos processos , aplicando ferramentas de qualidade , planejamento de demanda, intervenções operacionais, process mining , gestão da rotina , entre outras.\n",
    "Modelo de trabalho: híbrido de 8x presenciais no mês\n",
    "_Quais são as habilidades e competências necessárias?\n",
    "Principais Habilidades Técnicas Com Experiência Prática\n",
    "LEAN\n",
    "Qualidade\n",
    "PCP (Planejamento e Controle de Produção)\n",
    "Gestão de Projetos/ Métodos Ágeis\n",
    "Análise e exploração de Dados\n",
    "Process Mining\n",
    "Método de Solução de Problemas\n",
    "Formação em Engenharia de Produção é um diferencial\n",
    "_O que esperamos de você:\n",
    "Habilidade em criticar de forma construtiva e provocativa;\n",
    "Habilidade em negociação e construção de parcerias;\n",
    "Capacidade analítica/ lógica\n",
    "Habilidade em conectar conhecimento técnico com a estratégia dos projetos.\n",
    "Organização, disciplina e pró-atividade\n",
    "Habilidade em trabalhar em equipe de forma colaborativa,\n",
    "Capacidade de tomada de decisões com base em processos e dados, foco no cliente e resultados;\n",
    "🧡\n",
    "\"\"\"\n",
    "\n",
    "# Load Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Convert skill list into patterns\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Find matches\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "print(\"Extracted Skills:\", set(skills_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'arquitetura de dados', 'SQL', 'análise de dados', 'Ciência de Dados', 'Inteligência Artificial', 'Estatística'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process  # Fuzzy matching\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"Descrição\n",
    "Estamos em busca de um(a) estagiário(a) para integrar nosso time de dados, focado na automatização e análise de dados, contribuindo diretamente para a tomada de decisão estratégica da empresa.\n",
    "Responsabilidades e atribuições\n",
    "Atuar na construção, governança e manutenção da arquitetura de dados do RPA.\n",
    "Automatizar processo e otimização para reduzir esforços operacionais repetitivos.\n",
    "Promover a cultura de dados dentro da empresa e junto aos clientes, auxiliando em projetos em andamento.\n",
    "Participar da concessão, estruturação e organização de dados.\n",
    "Contribuir para projetos de Inteligência Artificial (IA).\n",
    "Apoiar a construção da arquitetura de dados e o desenvolvimento de novas funcionalidades que promovem escalabilidade e eficiência.\n",
    "Requisitos e qualificações\n",
    "Cursando graduação em Tecnologia, Engenharia, Estatística, Ciência de Dados ou áreas correlatas.\n",
    "Noções de SQL para manipulação e extração de dados.\n",
    "Familiaridade com ferramentas de BI (ex.: Metabase, powerbi).\n",
    "Programação em Phyton e uso de bibliotecas para análise de dados (diferencial).\n",
    "Conhecimento em ferramentas de RPA (diferencial).\n",
    "Capacidade de levantamento e acompanhamento de KPIs em colaboração com usuários.\"\"\"\n",
    "\n",
    "# Load Portuguese NLP model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Create PhraseMatcher for exact matches\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Extract exact matches using PhraseMatcher\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "# Extract all words from the text\n",
    "words = set(re.findall(r\"\\b\\w+\\b\", sample_desc))  # Get all unique words\n",
    "\n",
    "# Apply fuzzy matching for words with a similarity score above a threshold (80%)\n",
    "for word in words:\n",
    "    best_match, score = process.extractOne(word, skill_list)\n",
    "    if score > 93:  # Adjust threshold as needed\n",
    "        skills_found.append(best_match)\n",
    "\n",
    "# Remove duplicates\n",
    "skills_found = set(skills_found)\n",
    "\n",
    "print(\"Extracted Skills:\", skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python', 83)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne('phyton', skill_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
