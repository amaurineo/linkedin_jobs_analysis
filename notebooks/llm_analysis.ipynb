{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry run pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_NAME = \"Qwen/Qwen1.5-1.8B-Chat\"  # Smaller and instruction-tuned\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_qwen(job_description: str) -> dict:\n",
    "    \"\"\"Optimized extraction function with better prompt engineering\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at extracting required skills from job descriptions. \n",
    "    Given the job description below from a data related position, identify all the technical and professional skills mentioned. \n",
    "    Return the skills as a JSON object with the key \"skills\" containing a list of strings written in pt-br.\n",
    "    Do not include irrelevant information or explanations and only include skills that you consider essential to the data area.\n",
    "    \n",
    "\n",
    "    Job Description:\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": job_description[:2000]}\n",
    "    ]\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate output\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # Extract and clean output\n",
    "    raw_output = outputs[0]['generated_text'].strip()\n",
    "    json_str = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        if \"skills\" in result and isinstance(result[\"skills\"], list):\n",
    "            return result\n",
    "        else:\n",
    "            return {\"error\": \"Invalid JSON structure\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Descrição\\nEstamos em busca de um(a) estagiári...\n",
       "1    Lugar de gente inovadora que faz acontecer!\\nS...\n",
       "2    Há mais de 90 anos, o Banco Bmg vem construind...\n",
       "3    Somos uma indústria brasileira com atuação glo...\n",
       "4    About The Team\\nSobre a Shô\\nAqui na Shô, acre...\n",
       "5    Que tal construir a REDE com a gente? 🚀\\nHá 26...\n",
       "6    Descrição\\nO estagiário(a) de Sistemas da Info...\n",
       "7    Vem construir no Itaú_\\nComo maior banco priva...\n",
       "8    A NEON, ciente de sua responsabilidade social ...\n",
       "9    Somos uma empresa brasileira de varejo reconhe...\n",
       "Name: job_description, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')['job_description']\n",
    "job_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n",
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n",
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m job_descriptions:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result = \u001b[43mextract_requirements_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mextract_requirements_qwen\u001b[39m\u001b[34m(job_description)\u001b[39m\n\u001b[32m     21\u001b[39m prompt = tokenizer.apply_chat_template(\n\u001b[32m     22\u001b[39m     messages,\n\u001b[32m     23\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m outputs = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Extract and clean output\u001b[39;00m\n\u001b[32m     38\u001b[39m raw_output = outputs[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1364\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1365\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1368\u001b[39m         )\n\u001b[32m   1369\u001b[39m     )\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1378\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1377\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1278\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1277\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2318\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2319\u001b[39m         input_ids=input_ids,\n\u001b[32m   2320\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2321\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2322\u001b[39m         **model_kwargs,\n\u001b[32m   2323\u001b[39m     )\n\u001b[32m   2325\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2326\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2337\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2338\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2339\u001b[39m         input_ids=input_ids,\n\u001b[32m   2340\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2341\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2342\u001b[39m         **model_kwargs,\n\u001b[32m   2343\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3286\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3283\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3286\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3287\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:855\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    852\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    870\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:579\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    568\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    569\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m         position_embeddings,\n\u001b[32m    577\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:276\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m residual = hidden_states\n\u001b[32m    275\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    279\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:57\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for description in job_descriptions:\n",
    "    result = extract_requirements_qwen(description)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\", \n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Databases & Data Storage\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\", \n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\", \n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Data Processing & ETL\n",
    "    \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\", \n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\", \n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Data Build Tool)\", \"Great Expectations\", \n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Business Intelligence & Data Visualization\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\", \n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\", \n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \n",
    "\n",
    "    # Machine Learning & AI\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \n",
    "    \"Hugging Face Transformers\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\", \n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Weights & Biases\", \n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Natural Language Processing (NLP)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\", \n",
    "    \"SentenceTransformers\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Computer Vision\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\", \n",
    "    \"TensorFlow Object Detection API\", \"Albumentations\", \"TorchVision\",\n",
    "\n",
    "    # Big Data & Distributed Computing\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\", \n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps & Model Deployment\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\", \n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\", \n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Cloud Platforms\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\", \n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\", \n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Statistical & Mathematical Foundations\n",
    "    \"Linear Algebra\", \"Calculus\", \"Probability\", \"Bayesian Statistics\", \"Hypothesis Testing\", \n",
    "    \"Descriptive Statistics\", \"Inferential Statistics\", \"Optimization\", \"Time Series Analysis\", \n",
    "    \"Markov Chains\", \"Graph Theory\", \"Stochastic Processes\",\n",
    "\n",
    "    # Feature Engineering\n",
    "    \"Feature Selection\", \"Feature Scaling\", \"Dimensionality Reduction\", \"PCA\", \"t-SNE\", \n",
    "    \"UMAP\", \"One-Hot Encoding\", \"Label Encoding\", \"Ordinal Encoding\", \"Target Encoding\", \n",
    "    \"Feature Crossing\", \"Polynomial Features\",\n",
    "\n",
    "    # Model Explainability & Fairness\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facets\", \"Responsible AI Toolkit\", \n",
    "    \"What-If Tool\",\n",
    "\n",
    "    # Time Series Analysis\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Graph Data Science\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\", \n",
    "\n",
    "    # Reinforcement Learning\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Data Governance & Privacy\n",
    "    \"GDPR Compliance\", \"CCPA Compliance\", \"Data Masking\", \"Differential Privacy\", \"Anonymization\", \n",
    "    \"Data Lineage\", \"Data Cataloging\", \"Data Security\", \"Data Contracts\", \"Data Quality Management\",\n",
    "\n",
    "    # Data Engineering\n",
    "    \"ETL Pipelines\", \"Data Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\", \n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\", \n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentation & A/B Testing\n",
    "    \"A/B Testing\", \"Multivariate Testing\", \"Bandit Algorithms\", \"Bayesian Optimization\", \n",
    "    \"Causal Inference\", \"DoWhy\",\n",
    "\n",
    "    # Optimization & Operations Research\n",
    "    \"Linear Programming\", \"Mixed-Integer Programming\", \"Convex Optimization\", \"Simulated Annealing\", \n",
    "    \"Genetic Algorithms\", \"Constraint Programming\",\n",
    "\n",
    "    # Data Ethics & Bias\n",
    "    \"Algorithmic Fairness\", \"Bias Detection\", \"Interpretability\", \"Explainable AI (XAI)\", \"Fair AI\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Data Cleaning\", \"Data Transformation\", \n",
    "    \"Data Streaming\", \"Feature Store\", \"Real-Time Analytics\", \"DataOps\", \"Observability\",\n",
    "\n",
    "    # Portuguese Translations (Extra Entries)\n",
    "    \"Aprendizado de Máquina\", \"Engenharia de Dados\", \"Análise de Dados\", \"Processamento de Linguagem Natural\", \n",
    "    \"Ciência de Dados\", \"Visualização de Dados\", \"Estatística\", \"Otimização\", \"Big Data\", \n",
    "    \"Governança de Dados\", \"Orquestração de Dados\", \"Explicabilidade de Modelos\", \"Séries Temporais\",\n",
    "\n",
    "    # Linguagens de programação\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\",\n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Bancos de dados e armazenamento de dados\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\",\n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\",\n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Processamento de dados e ETL\n",
    "    \"Excel\", \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\",\n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\",\n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Ferramenta de construção de dados)\", \"Great Expectations\",\n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Inteligência de negócios e visualização de dados\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\",\n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\",\n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \"PowerPoint\", \"Word\"\n",
    "\n",
    "    # Aprendizado de máquina e IA\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\",\n",
    "    \"Transformadores de rostos abraçados\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\",\n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Pesos e vieses\",\n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Processamento de linguagem natural (PLN)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\",\n",
    "    \"Transformadores de frases\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Visão computacional\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\",\n",
    "    \"API de detecção de objetos TensorFlow\", \"Albumentações\", \"TorchVision\",\n",
    "\n",
    "    # Big Data e computação distribuída\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\",\n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps e implantação de modelo\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\",\n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\",\n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Plataformas de nuvem\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\",\n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\",\n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Fundamentos estatísticos e matemáticos\n",
    "    \"Álgebra linear\", \"Cálculo\", \"Probabilidade\", \"Estatística bayesiana\", \"Teste de hipóteses\",\n",
    "    \"Estatística descritiva\", \"Estatística inferencial\", \"Otimização\", \"Análise de séries temporais\",\n",
    "    \"Cadeias de Markov\", \"Teoria dos grafos\", \"Processos estocásticos\",\n",
    "\n",
    "    # Engenharia de recursos\n",
    "    \"Seleção de recursos\", \"Escalonamento de recursos\", \"Redução de dimensionalidade\", \"PCA\", \"t-SNE\",\n",
    "    \"UMAP\", \"Codificação One-Hot\", \"Codificação de rótulos\", \"Codificação ordinal\", \"Codificação de destino\",\n",
    "    \"Cruzamento de recursos\", \"Recursos polinomiais\",\n",
    "\n",
    "    # Explicabilidade e imparcialidade do modelo\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facetas\", \"Kit de ferramentas de IA responsável\",\n",
    "    \"Ferramenta What-If\",\n",
    "\n",
    "    # Análise de séries temporais\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Ciência de dados de gráfico\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\",\n",
    "\n",
    "    # Aprendizado por reforço\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Governança e privacidade de dados\n",
    "    \"Conformidade com GDPR\", \"Conformidade com CCPA\", \"Mascaramento de dados\", \"Privacidade diferencial\", \"Anonimização\",\n",
    "    \"Linhagem de dados\", \"Catalogação de dados\", \"Segurança de dados\", \"Contratos de dados\", \"Gerenciamento de qualidade de dados\",\n",
    "\n",
    "    # Engenharia de dados\n",
    "    \"Pipelines ETL\", \"Dados Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\",\n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\",\n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentação e Teste A/B\n",
    "    \"Teste A/B\", \"Teste Multivariado\", \"Algoritmos Bandit\", \"Otimização Bayesiana\",\n",
    "    \"Inferência Causal\", \"DoWhy\",\n",
    "\n",
    "    # Otimização e Pesquisa Operacional\n",
    "    \"Programação Linear\", \"Programação Inteira Mista\", \"Otimização Convexa\", \"Simulated Annealing\",\n",
    "    \"Algoritmos Genéticos\", \"Programação de Restrições\",\n",
    "\n",
    "    # Ética e Viés de Dados\n",
    "    \"Justiça Algorítmica\", \"Detecção de Viés\", \"Interpretabilidade\", \"IA Explicável (XAI)\", \"IA Justa\",\n",
    "\n",
    "    # Diversos\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Limpeza de dados\", \"Transformação de dados\", \n",
    "    \"Streaming de dados\", \"Feature Store\", \"Análise em tempo real\", \"DataOps\", \"Observabilidade\",\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages / Tools\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"SQL\",\n",
    "    \"Excel\",\n",
    "\n",
    "    # Machine Learning & Artificial Intelligence\n",
    "    \"Machine Learning\", \"Aprendizagem de Máquina\",\n",
    "    \"Deep Learning\", \"Aprendizagem Profunda\",\n",
    "    \"Reinforcement Learning\", \"Aprendizagem por Reforço\",\n",
    "    \"Artificial Intelligence\", \"Inteligência Artificial\",\n",
    "    \"Algorithm Development\", \"Desenvolvimento de Algoritmos\",\n",
    "\n",
    "    # Data Analysis & Statistics\n",
    "    \"Data Analysis\", \"Análise de Dados\",\n",
    "    \"Statistical Analysis\", \"Análise Estatística\",\n",
    "    \"Statistics\", \"Estatística\",\n",
    "    \"Predictive Analytics\", \"Análise Preditiva\",\n",
    "    \"Prescriptive Analytics\", \"Análise Prescritiva\",\n",
    "    \"Statistical Modeling\", \"Modelagem Estatística\",\n",
    "    \n",
    "    # Data Science & Engineering\n",
    "    \"Data Science\", \"Ciência de Dados\",\n",
    "    \"Data Engineering\", \"Engenharia de Dados\",\n",
    "    \"Data Architecture\", \"Arquitetura de Dados\",\n",
    "    \"Data Strategy\", \"Estratégia de Dados\",\n",
    "    \n",
    "    # Data Visualization & Storytelling\n",
    "    \"Data Visualization\", \"Visualização de Dados\",\n",
    "    \"Data Storytelling\", \"Contação de Histórias com Dados\",\n",
    "    \"Business Intelligence\", \"Inteligência de Negócios\",\n",
    "    \"Business Analytics\", \"Análise de Negócios\",\n",
    "    \n",
    "    # Big Data & Cloud\n",
    "    \"Big Data\", \"Big Data\",\n",
    "    \"Data Warehousing\", \"Armazenamento de Dados\",\n",
    "    \"Big Data Engineering\", \"Engenharia de Big Data\",\n",
    "    \"Big Data Analytics\", \"Análise de Big Data\",\n",
    "    \"Cloud Computing\", \"Computação em Nuvem\",\n",
    "    \"Cloud Data Services\", \"Serviços de Dados em Nuvem\",\n",
    "    \n",
    "    # Data Processing & Tools\n",
    "    \"ETL\",\n",
    "    \"Data Pipelines\", \"Fluxos de Dados\",\n",
    "    \"Data Integration\", \"Integração de Dados\",\n",
    "    \"Data Cleaning\", \"Limpeza de Dados\",\n",
    "    \"Data Wrangling\", \"Manipulação de Dados\",\n",
    "    \"Data Quality\", \"Qualidade dos Dados\",\n",
    "    \"Data Governance\", \"Governança de Dados\",\n",
    "    \"Data Platforms\", \"Plataformas de Dados\",\n",
    "    \n",
    "    # Databases & Query Optimization\n",
    "    \"NoSQL\",\n",
    "    \"SQL Tuning\", \"Otimização de SQL\",\n",
    "    \"Query Optimization\", \"Otimização de Consultas\",\n",
    "    \n",
    "    # Big Data Frameworks\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \n",
    "    # Specialized Tools & Software\n",
    "    \"Tableau\",\n",
    "    \"Power BI\",\n",
    "    \"SAS\",\n",
    "    \"Stata\",\n",
    "    \"Matlab\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \n",
    "    # Advanced Data Concepts\n",
    "    \"Feature Engineering\", \"Engenharia de Atributos\",\n",
    "    \"Time Series Analysis\", \"Análise de Séries Temporais\",\n",
    "    \"A/B Testing\", \"Testes A/B\",\n",
    "    \"Data Mining Techniques\", \"Técnicas de Mineração de Dados\",\n",
    "    \"Data Mining\", \"Mineração de Dados\",\n",
    "    \"Natural Language Processing\", \"Processamento de Linguagem Natural\",\n",
    "    \"Computer Vision\", \"Visão Computacional\",\n",
    "    \"Information Retrieval\", \"Recuperação de Informação\",\n",
    "    \n",
    "    # Security & Ethics\n",
    "    \"Data Security\", \"Segurança de Dados\",\n",
    "    \"Data Privacy\", \"Privacidade de Dados\",\n",
    "    \"Data Ethics\", \"Ética de Dados\"\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "# skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'Excel', 'PowerPoint', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"O estagiário(a) de Sistemas da Informação será responsável por prestar suporte aos usuários do ERP Datasul, atuando diretamente na resolução de problemas e na abertura e acompanhamento de chamados com a empresa fornecedora do software. O profissional também participará de treinamentos, reuniões de equipe.\n",
    "Responsabilidades e atribuições\n",
    "Prestar suporte técnico e funcional aos usuários do ERP Datasul.\n",
    "Auxiliar na resolução de problemas relacionados ao sistema, garantindo o bom funcionamento e a continuidade das operações.\n",
    "Realizar a abertura e acompanhamento de chamados técnicos junto à empresa fornecedora do software, garantindo que as solicitações sejam tratadas de maneira eficiente.\n",
    "Participar de treinamentos contínuos para atualização sobre novas funcionalidades e melhorias no sistema.\n",
    "Colaborar com a equipe nas reuniões para discutir melhorias, ajustes e necessidades do sistema.\n",
    "Atuar no suporte às áreas Contábil, Fiscal, Administrativa e Financeira, garantindo o alinhamento entre os processos e o ERP.\n",
    "Requisitos e qualificações\n",
    "Cursando graduação em Sistemas de Informação, Ciências da Computação ou áreas relacionadas.\n",
    "Conhecimento básico em sistemas ERP (preferencialmente Datasul).\n",
    "Habilidade em comunicação para interação com usuários e fornecedores.\n",
    "Proatividade, organização e vontade de aprender.\n",
    "Leitura de manuais, documentação e materiais técnicos para suporte ao usuário e resolução de problemas\n",
    "Entendimento do ciclo de vida e operação de sistemas computacionais.\n",
    "Noções básicas de bancos de dados relacionais e não relacionais.\n",
    "Conhecimentos básicos de consultas e manipulação de dados em SQL.\n",
    "Habilidade em ferramentas como Word, Excel e PowerPoint.\"\n",
    "7,4075025088,Faça sua Carreira de Engenharia de Processos no Itaú 🚀🧡,Itaú Unibanco,Híbrido,\"São Paulo, SP\",Há 2 semanas,,Assistente,Tempo integral,Bancos,\"Vem construir no Itaú_\n",
    "Como maior banco privado da América Latina, acompanhamos de perto o desenvolvimento de novas tecnologias e enxergamos o processo como uma evolução natural do nosso negócio, que\n",
    "torna possível termos um contato ainda mais próximo com a experiência de nossos clientes\n",
    "para continuarmos aprimorando suas jornadas.\n",
    "Como é a área de Operações?\n",
    "Ao longo de nossa história, desempenhamos um\n",
    "papel fundamental na estratégia do Itaú Unibanco\n",
    ". Dentro deste contexto, todos os produtos que oferecemos aos nossos clientes passam a ser produtos tecnológicos, independente da interface que ele escolha – seja ela física, digital ou mista. Vem construir soluções digitais em uma das maiores instituições financeiras da América Latina!\n",
    "Como é a Carreira de Engenharia de Processos?\n",
    "A Engenharia de processos no Itaú exerce um papel fundamental na busca constante da melhor jornada para nossos clientes, vem pra repensar o fundamental (ir na causa raiz das coisas,) buscar a reestruturação radical dos processos que visam alcançar mudanças disruptivas em indicadores críticos de desempenho, tais como qualidade, custos, riscos e agilidade. A pessoa engenheira de processos pode estar alocada em Squads, em comunidades ou nas áreas do banco atuando sempre em projetos de transformação e melhoria contínua (Build e Run).\n",
    "Como será o seu dia a dia?\n",
    "Garantir a aplicabilidade dos métodos para transformação das Jornadas fim a fim dos clientes.\n",
    "Priorizar as ações quantificando o impacto através de uma Engenharia de Valor, garantindo assertividade nas escolhas que agregam.\n",
    "Atuar no mapeamento de processos, resolução de problemas, construção e redesenho de jornadas, com foco no cliente, eficiência, qualidade e gestão de riscos.\n",
    "Desdobrar as dores dos clientes e conectar com elementos de processo.\n",
    "Realizar o estudo e a construção dos requisitos para os desenvolvimentos de tecnologia dos problemas assegurando o reuso de soluções.\n",
    "Desenvolver soluções em No/Low-Code para transformação dos processos. Garante o teste funcional e homologação de requisitos de processos nas soluções desenvolvidas\n",
    "Estrutura Gestão e Controle dos processos , aplicando ferramentas de qualidade , planejamento de demanda, intervenções operacionais, process mining , gestão da rotina , entre outras.\n",
    "Modelo de trabalho: híbrido de 8x presenciais no mês\n",
    "_Quais são as habilidades e competências necessárias?\n",
    "Principais Habilidades Técnicas Com Experiência Prática\n",
    "LEAN\n",
    "Qualidade\n",
    "PCP (Planejamento e Controle de Produção)\n",
    "Gestão de Projetos/ Métodos Ágeis\n",
    "Análise e exploração de Dados\n",
    "Process Mining\n",
    "Método de Solução de Problemas\n",
    "Formação em Engenharia de Produção é um diferencial\n",
    "_O que esperamos de você:\n",
    "Habilidade em criticar de forma construtiva e provocativa;\n",
    "Habilidade em negociação e construção de parcerias;\n",
    "Capacidade analítica/ lógica\n",
    "Habilidade em conectar conhecimento técnico com a estratégia dos projetos.\n",
    "Organização, disciplina e pró-atividade\n",
    "Habilidade em trabalhar em equipe de forma colaborativa,\n",
    "Capacidade de tomada de decisões com base em processos e dados, foco no cliente e resultados;\n",
    "🧡\n",
    "\"\"\"\n",
    "\n",
    "# Load Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Convert skill list into patterns\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Find matches\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "print(\"Extracted Skills:\", set(skills_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'arquitetura de dados', 'SQL', 'análise de dados', 'Ciência de Dados', 'Inteligência Artificial', 'Estatística'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process  # Fuzzy matching\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"Descrição\n",
    "Estamos em busca de um(a) estagiário(a) para integrar nosso time de dados, focado na automatização e análise de dados, contribuindo diretamente para a tomada de decisão estratégica da empresa.\n",
    "Responsabilidades e atribuições\n",
    "Atuar na construção, governança e manutenção da arquitetura de dados do RPA.\n",
    "Automatizar processo e otimização para reduzir esforços operacionais repetitivos.\n",
    "Promover a cultura de dados dentro da empresa e junto aos clientes, auxiliando em projetos em andamento.\n",
    "Participar da concessão, estruturação e organização de dados.\n",
    "Contribuir para projetos de Inteligência Artificial (IA).\n",
    "Apoiar a construção da arquitetura de dados e o desenvolvimento de novas funcionalidades que promovem escalabilidade e eficiência.\n",
    "Requisitos e qualificações\n",
    "Cursando graduação em Tecnologia, Engenharia, Estatística, Ciência de Dados ou áreas correlatas.\n",
    "Noções de SQL para manipulação e extração de dados.\n",
    "Familiaridade com ferramentas de BI (ex.: Metabase, powerbi).\n",
    "Programação em Phyton e uso de bibliotecas para análise de dados (diferencial).\n",
    "Conhecimento em ferramentas de RPA (diferencial).\n",
    "Capacidade de levantamento e acompanhamento de KPIs em colaboração com usuários.\"\"\"\n",
    "\n",
    "# Load Portuguese NLP model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Create PhraseMatcher for exact matches\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Extract exact matches using PhraseMatcher\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "# Extract all words from the text\n",
    "words = set(re.findall(r\"\\b\\w+\\b\", sample_desc))  # Get all unique words\n",
    "\n",
    "# Apply fuzzy matching for words with a similarity score above a threshold (80%)\n",
    "for word in words:\n",
    "    best_match, score = process.extractOne(word, skill_list)\n",
    "    if score > 93:  # Adjust threshold as needed\n",
    "        skills_found.append(best_match)\n",
    "\n",
    "# Remove duplicates\n",
    "skills_found = set(skills_found)\n",
    "\n",
    "print(\"Extracted Skills:\", skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python', 83)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne('phyton', skill_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
