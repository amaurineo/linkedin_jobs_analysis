{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry run pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_NAME = \"Qwen/Qwen1.5-1.8B-Chat\"  # Smaller and instruction-tuned\n",
    "MODEL_NAME = 'Qwen/Qwen2.5-7B'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.42s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_requirements_qwen(job_description: str) -> dict:\n",
    "    \"\"\"Optimized extraction function with better prompt engineering\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at extracting required skills from job descriptions. \n",
    "    Given the job description below from a data related position, identify all the technical and professional skills mentioned. \n",
    "    Return the skills as a JSON object with the key \"skills\" containing a list of strings written in pt-br.\n",
    "    Do not include irrelevant information or explanations and only include skills that you consider essential to the data area.\n",
    "    \n",
    "\n",
    "    Job Description:\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": job_description[:2000]}\n",
    "    ]\n",
    "\n",
    "    # Generate prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate output\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # Extract and clean output\n",
    "    raw_output = outputs[0]['generated_text'].strip()\n",
    "    json_str = raw_output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        if \"skills\" in result and isinstance(result[\"skills\"], list):\n",
    "            return result\n",
    "        else:\n",
    "            return {\"error\": \"Invalid JSON structure\"}\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DescriÃ§Ã£o\\nEstamos em busca de um(a) estagiÃ¡ri...\n",
       "1    Lugar de gente inovadora que faz acontecer!\\nS...\n",
       "2    HÃ¡ mais de 90 anos, o Banco Bmg vem construind...\n",
       "3    Somos uma indÃºstria brasileira com atuaÃ§Ã£o glo...\n",
       "4    About The Team\\nSobre a ShÃ´\\nAqui na ShÃ´, acre...\n",
       "5    Que tal construir a REDE com a gente? ðŸš€\\nHÃ¡ 26...\n",
       "6    DescriÃ§Ã£o\\nO estagiÃ¡rio(a) de Sistemas da Info...\n",
       "7    Vem construir no ItaÃº_\\nComo maior banco priva...\n",
       "8    A NEON, ciente de sua responsabilidade social ...\n",
       "9    Somos uma empresa brasileira de varejo reconhe...\n",
       "Name: job_description, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions = pd.read_csv('../data/raw/jobs_data.csv')['job_description']\n",
    "job_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n",
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n",
      "{\n",
      "  \"error\": \"Failed to parse JSON output\"\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m job_descriptions:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result = \u001b[43mextract_requirements_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mextract_requirements_qwen\u001b[39m\u001b[34m(job_description)\u001b[39m\n\u001b[32m     21\u001b[39m prompt = tokenizer.apply_chat_template(\n\u001b[32m     22\u001b[39m     messages,\n\u001b[32m     23\u001b[39m     tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m outputs = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Extract and clean output\u001b[39;00m\n\u001b[32m     38\u001b[39m raw_output = outputs[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1364\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1365\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1368\u001b[39m         )\n\u001b[32m   1369\u001b[39m     )\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1378\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1377\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1378\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1278\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1277\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2326\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2318\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2319\u001b[39m         input_ids=input_ids,\n\u001b[32m   2320\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2321\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2322\u001b[39m         **model_kwargs,\n\u001b[32m   2323\u001b[39m     )\n\u001b[32m   2325\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2326\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2337\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2338\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2339\u001b[39m         input_ids=input_ids,\n\u001b[32m   2340\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2341\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2342\u001b[39m         **model_kwargs,\n\u001b[32m   2343\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3286\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3283\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3286\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3287\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:855\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    852\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    870\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:579\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    568\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    569\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m         position_embeddings,\n\u001b[32m    577\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:276\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m residual = hidden_states\n\u001b[32m    275\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    279\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:57\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pschm\\OneDrive\\eng\\projetos\\linkedin_jobs_analysis\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for description in job_descriptions:\n",
    "    result = extract_requirements_qwen(description)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\", \n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Databases & Data Storage\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\", \n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\", \n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Data Processing & ETL\n",
    "    \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\", \n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\", \n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Data Build Tool)\", \"Great Expectations\", \n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # Business Intelligence & Data Visualization\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\", \n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\", \n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \n",
    "\n",
    "    # Machine Learning & AI\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \n",
    "    \"Hugging Face Transformers\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\", \n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Weights & Biases\", \n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Natural Language Processing (NLP)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\", \n",
    "    \"SentenceTransformers\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # Computer Vision\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\", \n",
    "    \"TensorFlow Object Detection API\", \"Albumentations\", \"TorchVision\",\n",
    "\n",
    "    # Big Data & Distributed Computing\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\", \n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps & Model Deployment\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\", \n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\", \n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Cloud Platforms\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\", \n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\", \n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Statistical & Mathematical Foundations\n",
    "    \"Linear Algebra\", \"Calculus\", \"Probability\", \"Bayesian Statistics\", \"Hypothesis Testing\", \n",
    "    \"Descriptive Statistics\", \"Inferential Statistics\", \"Optimization\", \"Time Series Analysis\", \n",
    "    \"Markov Chains\", \"Graph Theory\", \"Stochastic Processes\",\n",
    "\n",
    "    # Feature Engineering\n",
    "    \"Feature Selection\", \"Feature Scaling\", \"Dimensionality Reduction\", \"PCA\", \"t-SNE\", \n",
    "    \"UMAP\", \"One-Hot Encoding\", \"Label Encoding\", \"Ordinal Encoding\", \"Target Encoding\", \n",
    "    \"Feature Crossing\", \"Polynomial Features\",\n",
    "\n",
    "    # Model Explainability & Fairness\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facets\", \"Responsible AI Toolkit\", \n",
    "    \"What-If Tool\",\n",
    "\n",
    "    # Time Series Analysis\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # Graph Data Science\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\", \n",
    "\n",
    "    # Reinforcement Learning\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # Data Governance & Privacy\n",
    "    \"GDPR Compliance\", \"CCPA Compliance\", \"Data Masking\", \"Differential Privacy\", \"Anonymization\", \n",
    "    \"Data Lineage\", \"Data Cataloging\", \"Data Security\", \"Data Contracts\", \"Data Quality Management\",\n",
    "\n",
    "    # Data Engineering\n",
    "    \"ETL Pipelines\", \"Data Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\", \n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\", \n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # Experimentation & A/B Testing\n",
    "    \"A/B Testing\", \"Multivariate Testing\", \"Bandit Algorithms\", \"Bayesian Optimization\", \n",
    "    \"Causal Inference\", \"DoWhy\",\n",
    "\n",
    "    # Optimization & Operations Research\n",
    "    \"Linear Programming\", \"Mixed-Integer Programming\", \"Convex Optimization\", \"Simulated Annealing\", \n",
    "    \"Genetic Algorithms\", \"Constraint Programming\",\n",
    "\n",
    "    # Data Ethics & Bias\n",
    "    \"Algorithmic Fairness\", \"Bias Detection\", \"Interpretability\", \"Explainable AI (XAI)\", \"Fair AI\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Data Cleaning\", \"Data Transformation\", \n",
    "    \"Data Streaming\", \"Feature Store\", \"Real-Time Analytics\", \"DataOps\", \"Observability\",\n",
    "\n",
    "    # Portuguese Translations (Extra Entries)\n",
    "    \"Aprendizado de MÃ¡quina\", \"Engenharia de Dados\", \"AnÃ¡lise de Dados\", \"Processamento de Linguagem Natural\", \n",
    "    \"CiÃªncia de Dados\", \"VisualizaÃ§Ã£o de Dados\", \"EstatÃ­stica\", \"OtimizaÃ§Ã£o\", \"Big Data\", \n",
    "    \"GovernanÃ§a de Dados\", \"OrquestraÃ§Ã£o de Dados\", \"Explicabilidade de Modelos\", \"SÃ©ries Temporais\",\n",
    "\n",
    "    # Linguagens de programaÃ§Ã£o\n",
    "    \"Python\", \"R\", \"SQL\", \"Julia\", \"Scala\", \"Java\", \"C++\", \"MATLAB\", \"SAS\",\n",
    "    \"JavaScript\", \"TypeScript\", \"Bash\", \"Go\", \"Rust\",\n",
    "\n",
    "    # Bancos de dados e armazenamento de dados\n",
    "    \"PostgreSQL\", \"MySQL\", \"SQLite\", \"SQL Server\", \"Oracle Database\", \"MongoDB\",\n",
    "    \"Cassandra\", \"Redis\", \"Elasticsearch\", \"InfluxDB\", \"Neo4j\", \"DynamoDB\", \"BigQuery\",\n",
    "    \"Snowflake\", \"Redshift\", \"ClickHouse\", \"DuckDB\", \"MariaDB\", \"Firebase Realtime Database\",\n",
    "\n",
    "    # Processamento de dados e ETL\n",
    "    \"Excel\", \"Pandas\", \"Polars\", \"Dask\", \"Modin\", \"Vaex\", \"Koalas\", \"PySpark\", \"Apache Spark\",\n",
    "    \"Apache Flink\", \"Apache Beam\", \"Airflow\", \"Luigi\", \"Prefect\", \"Kubernetes\",\n",
    "    \"Docker\", \"Kafka\", \"RabbitMQ\", \"Celery\", \"DBT (Ferramenta de construÃ§Ã£o de dados)\", \"Great Expectations\",\n",
    "    \"Hadoop\", \"Databricks\", \"Azure Data Factory\", \"AWS Glue\",\n",
    "\n",
    "    # InteligÃªncia de negÃ³cios e visualizaÃ§Ã£o de dados\n",
    "    \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Qlik Sense\", \"Metabase\",\n",
    "    \"Superset\", \"Dash\", \"Streamlit\", \"Plotly\", \"Matplotlib\", \"Seaborn\", \"Bokeh\",\n",
    "    \"Altair\", \"ggplot2\", \"Grafana\", \"D3.js\", \"ECharts\", \"PowerPoint\", \"Word\"\n",
    "\n",
    "    # Aprendizado de mÃ¡quina e IA\n",
    "    \"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"XGBoost\", \"LightGBM\", \"CatBoost\",\n",
    "    \"Transformadores de rostos abraÃ§ados\", \"OpenCV\", \"DeepFace\", \"Numpy\", \"Scipy\", \"Statsmodels\",\n",
    "    \"Fastai\", \"EvidentlyAI\", \"Optuna\", \"Hyperopt\", \"Ray Tune\", \"MLflow\", \"Pesos e vieses\",\n",
    "    \"AutoML\", \"H2O.ai\", \"PyCaret\", \"TPOT\", \"Turi Create\",\n",
    "\n",
    "    # Processamento de linguagem natural (PLN)\n",
    "    \"NLTK\", \"spaCy\", \"Gensim\", \"TextBlob\", \"BERT\", \"GPT\", \"Word2Vec\", \"FastText\",\n",
    "    \"Transformadores de frases\", \"LlamaIndex\", \"Haystack\", \"Rasa\", \"LangChain\", \"Llama\",\n",
    "\n",
    "    # VisÃ£o computacional\n",
    "    \"OpenCV\", \"Detectron2\", \"YOLO\", \"MMDetection\", \"DeepFace\", \"Mediapipe\", \"Dlib\",\n",
    "    \"API de detecÃ§Ã£o de objetos TensorFlow\", \"AlbumentaÃ§Ãµes\", \"TorchVision\",\n",
    "\n",
    "    # Big Data e computaÃ§Ã£o distribuÃ­da\n",
    "    \"Apache Spark\", \"Dask\", \"Ray\", \"Apache Flink\", \"Hadoop\", \"Flink\", \"Google BigQuery\",\n",
    "    \"Presto\", \"Trino\", \"ClickHouse\", \"Hive\", \"HBase\",\n",
    "\n",
    "    # MLOps e implantaÃ§Ã£o de modelo\n",
    "    \"MLflow\", \"TensorFlow Serving\", \"TorchServe\", \"Kubeflow\", \"Seldon\", \"BentoML\",\n",
    "    \"FastAPI\", \"Flask\", \"Streamlit\", \"Triton Inference Server\", \"ONNX\", \"AWS SageMaker\",\n",
    "    \"Azure ML\", \"Google Vertex AI\", \"Ray Serve\",\n",
    "\n",
    "    # Plataformas de nuvem\n",
    "    \"AWS\", \"Azure\", \"Google Cloud Platform\", \"IBM Cloud\", \"Oracle Cloud\",\n",
    "    \"Snowflake\", \"Databricks\", \"Redshift\", \"BigQuery\", \"DataBricks\", \"Terraform\",\n",
    "    \"Kubernetes\", \"Docker\",\n",
    "\n",
    "    # Fundamentos estatÃ­sticos e matemÃ¡ticos\n",
    "    \"Ãlgebra linear\", \"CÃ¡lculo\", \"Probabilidade\", \"EstatÃ­stica bayesiana\", \"Teste de hipÃ³teses\",\n",
    "    \"EstatÃ­stica descritiva\", \"EstatÃ­stica inferencial\", \"OtimizaÃ§Ã£o\", \"AnÃ¡lise de sÃ©ries temporais\",\n",
    "    \"Cadeias de Markov\", \"Teoria dos grafos\", \"Processos estocÃ¡sticos\",\n",
    "\n",
    "    # Engenharia de recursos\n",
    "    \"SeleÃ§Ã£o de recursos\", \"Escalonamento de recursos\", \"ReduÃ§Ã£o de dimensionalidade\", \"PCA\", \"t-SNE\",\n",
    "    \"UMAP\", \"CodificaÃ§Ã£o One-Hot\", \"CodificaÃ§Ã£o de rÃ³tulos\", \"CodificaÃ§Ã£o ordinal\", \"CodificaÃ§Ã£o de destino\",\n",
    "    \"Cruzamento de recursos\", \"Recursos polinomiais\",\n",
    "\n",
    "    # Explicabilidade e imparcialidade do modelo\n",
    "    \"SHAP\", \"LIME\", \"Eli5\", \"Fairlearn\", \"Aequitas\", \"Facetas\", \"Kit de ferramentas de IA responsÃ¡vel\",\n",
    "    \"Ferramenta What-If\",\n",
    "\n",
    "    # AnÃ¡lise de sÃ©ries temporais\n",
    "    \"ARIMA\", \"SARIMA\", \"Prophet\", \"LSTM\", \"Holt-Winters\", \"VAR\", \"TBATS\", \"Kats\", \"GluonTS\",\n",
    "\n",
    "    # CiÃªncia de dados de grÃ¡fico\n",
    "    \"NetworkX\", \"Neo4j\", \"GraphFrames\", \"GraphX\", \"DGL (Deep Graph Library)\", \"PyG (PyTorch Geometric)\",\n",
    "\n",
    "    # Aprendizado por reforÃ§o\n",
    "    \"Stable-Baselines3\", \"RLlib\", \"Gym\", \"Mujoco\", \"PettingZoo\", \"Acme\",\n",
    "\n",
    "    # GovernanÃ§a e privacidade de dados\n",
    "    \"Conformidade com GDPR\", \"Conformidade com CCPA\", \"Mascaramento de dados\", \"Privacidade diferencial\", \"AnonimizaÃ§Ã£o\",\n",
    "    \"Linhagem de dados\", \"CatalogaÃ§Ã£o de dados\", \"SeguranÃ§a de dados\", \"Contratos de dados\", \"Gerenciamento de qualidade de dados\",\n",
    "\n",
    "    # Engenharia de dados\n",
    "    \"Pipelines ETL\", \"Dados Warehousing\", \"Data Lakes\", \"Delta Lake\", \"Lakehouse Architecture\",\n",
    "    \"OLAP vs OLTP\", \"CDC (Change Data Capture)\", \"Data Orchestration\", \"Message Queues (Kafka, RabbitMQ)\",\n",
    "    \"Reverse ETL\",\n",
    "\n",
    "    # ExperimentaÃ§Ã£o e Teste A/B\n",
    "    \"Teste A/B\", \"Teste Multivariado\", \"Algoritmos Bandit\", \"OtimizaÃ§Ã£o Bayesiana\",\n",
    "    \"InferÃªncia Causal\", \"DoWhy\",\n",
    "\n",
    "    # OtimizaÃ§Ã£o e Pesquisa Operacional\n",
    "    \"ProgramaÃ§Ã£o Linear\", \"ProgramaÃ§Ã£o Inteira Mista\", \"OtimizaÃ§Ã£o Convexa\", \"Simulated Annealing\",\n",
    "    \"Algoritmos GenÃ©ticos\", \"ProgramaÃ§Ã£o de RestriÃ§Ãµes\",\n",
    "\n",
    "    # Ã‰tica e ViÃ©s de Dados\n",
    "    \"JustiÃ§a AlgorÃ­tmica\", \"DetecÃ§Ã£o de ViÃ©s\", \"Interpretabilidade\", \"IA ExplicÃ¡vel (XAI)\", \"IA Justa\",\n",
    "\n",
    "    # Diversos\n",
    "    \"Regex\", \"Web Scraping\", \"BeautifulSoup\", \"Scrapy\", \"Limpeza de dados\", \"TransformaÃ§Ã£o de dados\", \n",
    "    \"Streaming de dados\", \"Feature Store\", \"AnÃ¡lise em tempo real\", \"DataOps\", \"Observabilidade\",\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_list = [\n",
    "    # Programming Languages / Tools\n",
    "    \"Python\",\n",
    "    \"R\",\n",
    "    \"SQL\",\n",
    "    \"Excel\",\n",
    "\n",
    "    # Machine Learning & Artificial Intelligence\n",
    "    \"Machine Learning\", \"Aprendizagem de MÃ¡quina\",\n",
    "    \"Deep Learning\", \"Aprendizagem Profunda\",\n",
    "    \"Reinforcement Learning\", \"Aprendizagem por ReforÃ§o\",\n",
    "    \"Artificial Intelligence\", \"InteligÃªncia Artificial\",\n",
    "    \"Algorithm Development\", \"Desenvolvimento de Algoritmos\",\n",
    "\n",
    "    # Data Analysis & Statistics\n",
    "    \"Data Analysis\", \"AnÃ¡lise de Dados\",\n",
    "    \"Statistical Analysis\", \"AnÃ¡lise EstatÃ­stica\",\n",
    "    \"Statistics\", \"EstatÃ­stica\",\n",
    "    \"Predictive Analytics\", \"AnÃ¡lise Preditiva\",\n",
    "    \"Prescriptive Analytics\", \"AnÃ¡lise Prescritiva\",\n",
    "    \"Statistical Modeling\", \"Modelagem EstatÃ­stica\",\n",
    "    \n",
    "    # Data Science & Engineering\n",
    "    \"Data Science\", \"CiÃªncia de Dados\",\n",
    "    \"Data Engineering\", \"Engenharia de Dados\",\n",
    "    \"Data Architecture\", \"Arquitetura de Dados\",\n",
    "    \"Data Strategy\", \"EstratÃ©gia de Dados\",\n",
    "    \n",
    "    # Data Visualization & Storytelling\n",
    "    \"Data Visualization\", \"VisualizaÃ§Ã£o de Dados\",\n",
    "    \"Data Storytelling\", \"ContaÃ§Ã£o de HistÃ³rias com Dados\",\n",
    "    \"Business Intelligence\", \"InteligÃªncia de NegÃ³cios\",\n",
    "    \"Business Analytics\", \"AnÃ¡lise de NegÃ³cios\",\n",
    "    \n",
    "    # Big Data & Cloud\n",
    "    \"Big Data\", \"Big Data\",\n",
    "    \"Data Warehousing\", \"Armazenamento de Dados\",\n",
    "    \"Big Data Engineering\", \"Engenharia de Big Data\",\n",
    "    \"Big Data Analytics\", \"AnÃ¡lise de Big Data\",\n",
    "    \"Cloud Computing\", \"ComputaÃ§Ã£o em Nuvem\",\n",
    "    \"Cloud Data Services\", \"ServiÃ§os de Dados em Nuvem\",\n",
    "    \n",
    "    # Data Processing & Tools\n",
    "    \"ETL\",\n",
    "    \"Data Pipelines\", \"Fluxos de Dados\",\n",
    "    \"Data Integration\", \"IntegraÃ§Ã£o de Dados\",\n",
    "    \"Data Cleaning\", \"Limpeza de Dados\",\n",
    "    \"Data Wrangling\", \"ManipulaÃ§Ã£o de Dados\",\n",
    "    \"Data Quality\", \"Qualidade dos Dados\",\n",
    "    \"Data Governance\", \"GovernanÃ§a de Dados\",\n",
    "    \"Data Platforms\", \"Plataformas de Dados\",\n",
    "    \n",
    "    # Databases & Query Optimization\n",
    "    \"NoSQL\",\n",
    "    \"SQL Tuning\", \"OtimizaÃ§Ã£o de SQL\",\n",
    "    \"Query Optimization\", \"OtimizaÃ§Ã£o de Consultas\",\n",
    "    \n",
    "    # Big Data Frameworks\n",
    "    \"Hadoop\",\n",
    "    \"Spark\",\n",
    "    \n",
    "    # Specialized Tools & Software\n",
    "    \"Tableau\",\n",
    "    \"Power BI\",\n",
    "    \"SAS\",\n",
    "    \"Stata\",\n",
    "    \"Matlab\",\n",
    "    \"Docker\",\n",
    "    \"Kubernetes\",\n",
    "    \n",
    "    # Advanced Data Concepts\n",
    "    \"Feature Engineering\", \"Engenharia de Atributos\",\n",
    "    \"Time Series Analysis\", \"AnÃ¡lise de SÃ©ries Temporais\",\n",
    "    \"A/B Testing\", \"Testes A/B\",\n",
    "    \"Data Mining Techniques\", \"TÃ©cnicas de MineraÃ§Ã£o de Dados\",\n",
    "    \"Data Mining\", \"MineraÃ§Ã£o de Dados\",\n",
    "    \"Natural Language Processing\", \"Processamento de Linguagem Natural\",\n",
    "    \"Computer Vision\", \"VisÃ£o Computacional\",\n",
    "    \"Information Retrieval\", \"RecuperaÃ§Ã£o de InformaÃ§Ã£o\",\n",
    "    \n",
    "    # Security & Ethics\n",
    "    \"Data Security\", \"SeguranÃ§a de Dados\",\n",
    "    \"Data Privacy\", \"Privacidade de Dados\",\n",
    "    \"Data Ethics\", \"Ã‰tica de Dados\"\n",
    "]\n",
    "skill_list = list(set(skill_list))\n",
    "# skill_list = [x.lower() for x in skill_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'Excel', 'PowerPoint', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"O estagiÃ¡rio(a) de Sistemas da InformaÃ§Ã£o serÃ¡ responsÃ¡vel por prestar suporte aos usuÃ¡rios do ERP Datasul, atuando diretamente na resoluÃ§Ã£o de problemas e na abertura e acompanhamento de chamados com a empresa fornecedora do software. O profissional tambÃ©m participarÃ¡ de treinamentos, reuniÃµes de equipe.\n",
    "Responsabilidades e atribuiÃ§Ãµes\n",
    "Prestar suporte tÃ©cnico e funcional aos usuÃ¡rios do ERP Datasul.\n",
    "Auxiliar na resoluÃ§Ã£o de problemas relacionados ao sistema, garantindo o bom funcionamento e a continuidade das operaÃ§Ãµes.\n",
    "Realizar a abertura e acompanhamento de chamados tÃ©cnicos junto Ã  empresa fornecedora do software, garantindo que as solicitaÃ§Ãµes sejam tratadas de maneira eficiente.\n",
    "Participar de treinamentos contÃ­nuos para atualizaÃ§Ã£o sobre novas funcionalidades e melhorias no sistema.\n",
    "Colaborar com a equipe nas reuniÃµes para discutir melhorias, ajustes e necessidades do sistema.\n",
    "Atuar no suporte Ã s Ã¡reas ContÃ¡bil, Fiscal, Administrativa e Financeira, garantindo o alinhamento entre os processos e o ERP.\n",
    "Requisitos e qualificaÃ§Ãµes\n",
    "Cursando graduaÃ§Ã£o em Sistemas de InformaÃ§Ã£o, CiÃªncias da ComputaÃ§Ã£o ou Ã¡reas relacionadas.\n",
    "Conhecimento bÃ¡sico em sistemas ERP (preferencialmente Datasul).\n",
    "Habilidade em comunicaÃ§Ã£o para interaÃ§Ã£o com usuÃ¡rios e fornecedores.\n",
    "Proatividade, organizaÃ§Ã£o e vontade de aprender.\n",
    "Leitura de manuais, documentaÃ§Ã£o e materiais tÃ©cnicos para suporte ao usuÃ¡rio e resoluÃ§Ã£o de problemas\n",
    "Entendimento do ciclo de vida e operaÃ§Ã£o de sistemas computacionais.\n",
    "NoÃ§Ãµes bÃ¡sicas de bancos de dados relacionais e nÃ£o relacionais.\n",
    "Conhecimentos bÃ¡sicos de consultas e manipulaÃ§Ã£o de dados em SQL.\n",
    "Habilidade em ferramentas como Word, Excel e PowerPoint.\"\n",
    "7,4075025088,FaÃ§a sua Carreira de Engenharia de Processos no ItaÃº ðŸš€ðŸ§¡,ItaÃº Unibanco,HÃ­brido,\"SÃ£o Paulo, SP\",HÃ¡ 2 semanas,,Assistente,Tempo integral,Bancos,\"Vem construir no ItaÃº_\n",
    "Como maior banco privado da AmÃ©rica Latina, acompanhamos de perto o desenvolvimento de novas tecnologias e enxergamos o processo como uma evoluÃ§Ã£o natural do nosso negÃ³cio, que\n",
    "torna possÃ­vel termos um contato ainda mais prÃ³ximo com a experiÃªncia de nossos clientes\n",
    "para continuarmos aprimorando suas jornadas.\n",
    "Como Ã© a Ã¡rea de OperaÃ§Ãµes?\n",
    "Ao longo de nossa histÃ³ria, desempenhamos um\n",
    "papel fundamental na estratÃ©gia do ItaÃº Unibanco\n",
    ". Dentro deste contexto, todos os produtos que oferecemos aos nossos clientes passam a ser produtos tecnolÃ³gicos, independente da interface que ele escolha â€“ seja ela fÃ­sica, digital ou mista. Vem construir soluÃ§Ãµes digitais em uma das maiores instituiÃ§Ãµes financeiras da AmÃ©rica Latina!\n",
    "Como Ã© a Carreira de Engenharia de Processos?\n",
    "A Engenharia de processos no ItaÃº exerce um papel fundamental na busca constante da melhor jornada para nossos clientes, vem pra repensar o fundamental (ir na causa raiz das coisas,) buscar a reestruturaÃ§Ã£o radical dos processos que visam alcanÃ§ar mudanÃ§as disruptivas em indicadores crÃ­ticos de desempenho, tais como qualidade, custos, riscos e agilidade. A pessoa engenheira de processos pode estar alocada em Squads, em comunidades ou nas Ã¡reas do banco atuando sempre em projetos de transformaÃ§Ã£o e melhoria contÃ­nua (Build e Run).\n",
    "Como serÃ¡ o seu dia a dia?\n",
    "Garantir a aplicabilidade dos mÃ©todos para transformaÃ§Ã£o das Jornadas fim a fim dos clientes.\n",
    "Priorizar as aÃ§Ãµes quantificando o impacto atravÃ©s de uma Engenharia de Valor, garantindo assertividade nas escolhas que agregam.\n",
    "Atuar no mapeamento de processos, resoluÃ§Ã£o de problemas, construÃ§Ã£o e redesenho de jornadas, com foco no cliente, eficiÃªncia, qualidade e gestÃ£o de riscos.\n",
    "Desdobrar as dores dos clientes e conectar com elementos de processo.\n",
    "Realizar o estudo e a construÃ§Ã£o dos requisitos para os desenvolvimentos de tecnologia dos problemas assegurando o reuso de soluÃ§Ãµes.\n",
    "Desenvolver soluÃ§Ãµes em No/Low-Code para transformaÃ§Ã£o dos processos. Garante o teste funcional e homologaÃ§Ã£o de requisitos de processos nas soluÃ§Ãµes desenvolvidas\n",
    "Estrutura GestÃ£o e Controle dos processos , aplicando ferramentas de qualidade , planejamento de demanda, intervenÃ§Ãµes operacionais, process mining , gestÃ£o da rotina , entre outras.\n",
    "Modelo de trabalho: hÃ­brido de 8x presenciais no mÃªs\n",
    "_Quais sÃ£o as habilidades e competÃªncias necessÃ¡rias?\n",
    "Principais Habilidades TÃ©cnicas Com ExperiÃªncia PrÃ¡tica\n",
    "LEAN\n",
    "Qualidade\n",
    "PCP (Planejamento e Controle de ProduÃ§Ã£o)\n",
    "GestÃ£o de Projetos/ MÃ©todos Ãgeis\n",
    "AnÃ¡lise e exploraÃ§Ã£o de Dados\n",
    "Process Mining\n",
    "MÃ©todo de SoluÃ§Ã£o de Problemas\n",
    "FormaÃ§Ã£o em Engenharia de ProduÃ§Ã£o Ã© um diferencial\n",
    "_O que esperamos de vocÃª:\n",
    "Habilidade em criticar de forma construtiva e provocativa;\n",
    "Habilidade em negociaÃ§Ã£o e construÃ§Ã£o de parcerias;\n",
    "Capacidade analÃ­tica/ lÃ³gica\n",
    "Habilidade em conectar conhecimento tÃ©cnico com a estratÃ©gia dos projetos.\n",
    "OrganizaÃ§Ã£o, disciplina e prÃ³-atividade\n",
    "Habilidade em trabalhar em equipe de forma colaborativa,\n",
    "Capacidade de tomada de decisÃµes com base em processos e dados, foco no cliente e resultados;\n",
    "ðŸ§¡\n",
    "\"\"\"\n",
    "\n",
    "# Load Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Convert skill list into patterns\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Find matches\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "print(\"Extracted Skills:\", set(skills_found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: {'arquitetura de dados', 'SQL', 'anÃ¡lise de dados', 'CiÃªncia de Dados', 'InteligÃªncia Artificial', 'EstatÃ­stica'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from thefuzz import process  # Fuzzy matching\n",
    "import re\n",
    "\n",
    "sample_desc = \"\"\"DescriÃ§Ã£o\n",
    "Estamos em busca de um(a) estagiÃ¡rio(a) para integrar nosso time de dados, focado na automatizaÃ§Ã£o e anÃ¡lise de dados, contribuindo diretamente para a tomada de decisÃ£o estratÃ©gica da empresa.\n",
    "Responsabilidades e atribuiÃ§Ãµes\n",
    "Atuar na construÃ§Ã£o, governanÃ§a e manutenÃ§Ã£o da arquitetura de dados do RPA.\n",
    "Automatizar processo e otimizaÃ§Ã£o para reduzir esforÃ§os operacionais repetitivos.\n",
    "Promover a cultura de dados dentro da empresa e junto aos clientes, auxiliando em projetos em andamento.\n",
    "Participar da concessÃ£o, estruturaÃ§Ã£o e organizaÃ§Ã£o de dados.\n",
    "Contribuir para projetos de InteligÃªncia Artificial (IA).\n",
    "Apoiar a construÃ§Ã£o da arquitetura de dados e o desenvolvimento de novas funcionalidades que promovem escalabilidade e eficiÃªncia.\n",
    "Requisitos e qualificaÃ§Ãµes\n",
    "Cursando graduaÃ§Ã£o em Tecnologia, Engenharia, EstatÃ­stica, CiÃªncia de Dados ou Ã¡reas correlatas.\n",
    "NoÃ§Ãµes de SQL para manipulaÃ§Ã£o e extraÃ§Ã£o de dados.\n",
    "Familiaridade com ferramentas de BI (ex.: Metabase, powerbi).\n",
    "ProgramaÃ§Ã£o em Phyton e uso de bibliotecas para anÃ¡lise de dados (diferencial).\n",
    "Conhecimento em ferramentas de RPA (diferencial).\n",
    "Capacidade de levantamento e acompanhamento de KPIs em colaboraÃ§Ã£o com usuÃ¡rios.\"\"\"\n",
    "\n",
    "# Load Portuguese NLP model\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Create PhraseMatcher for exact matches\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp(skill) for skill in skill_list]\n",
    "matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "# Process text\n",
    "doc = nlp(sample_desc)\n",
    "\n",
    "# Extract exact matches using PhraseMatcher\n",
    "matches = matcher(doc)\n",
    "skills_found = [doc[start:end].text for match_id, start, end in matches]\n",
    "\n",
    "# Extract all words from the text\n",
    "words = set(re.findall(r\"\\b\\w+\\b\", sample_desc))  # Get all unique words\n",
    "\n",
    "# Apply fuzzy matching for words with a similarity score above a threshold (80%)\n",
    "for word in words:\n",
    "    best_match, score = process.extractOne(word, skill_list)\n",
    "    if score > 93:  # Adjust threshold as needed\n",
    "        skills_found.append(best_match)\n",
    "\n",
    "# Remove duplicates\n",
    "skills_found = set(skills_found)\n",
    "\n",
    "print(\"Extracted Skills:\", skills_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Python', 83)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.extractOne('phyton', skill_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
